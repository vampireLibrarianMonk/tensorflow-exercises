{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d08a95-3d3b-46e6-97a6-a90f7878dde1",
   "metadata": {},
   "source": [
    "Quantization involves reducing the precision of your model's weights and, optionally, activation functions from floating-point numbers (like float32) to lower-bit representations, such as int8 or float16. This process can significantly reduce the model size and speed up inference, making it highly suitable for deployment on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae9fb5-e487-4830-a166-4e5db484aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:15:18.210714: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-27 20:15:18.228962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-27 20:15:18.228976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-27 20:15:18.229467: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-27 20:15:18.233291: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 20:15:18.559553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and\n",
    "# matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Importing specific modules from keras, which is now part of TensorFlow\n",
    "# Callbacks are utilities called at certain points during model training. EarlyStopping stops training when a monitored\n",
    "# metric has stopped improving, and ModelCheckpoint saves the model after every epoch.\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# load_model is used to load a saved model. Sequential is a linear stack of layers.\n",
    "from keras.models import Sequential\n",
    "# Dense is a standard layer type that is used in many neural networks.\n",
    "from keras.layers import Dense\n",
    "\n",
    "# TensorFlow Lite provides tools and classes for converting TensorFlow models into a highly optimized format suitable\n",
    "# for deployment on mobile devices, embedded systems, or other platforms with limited computational capacity. This\n",
    "# module includes functionalities for model conversion, optimization, and inference. By importing `lite`, you gain\n",
    "# access to the TFLiteConverter class for model conversion, optimization options like quantization, and utilities for\n",
    "# running TFLite models on target devices.\n",
    "from tensorflow import lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3ec0e0-3ca7-473c-90bc-488b4d435533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:15:18.883576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21880 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequential model with a single Dense (fully connected) layer.\n",
    "# The model will have a single input feature and a linear activation function.\n",
    "model = Sequential([\n",
    "    Dense(units=1, input_shape=[1], activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5da0edf-684b-41c1-90f8-4f7b39bf028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the Stochastic Gradient Descent (SGD) optimizer.\n",
    "# Use mean squared error as the loss function, suitable for regression problems.\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3e9b7e-85d1-4ef7-a1fe-c057db87fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for training.\n",
    "# `xs` represents the input features, and `ys` represents the target outputs.\n",
    "# These arrays are used to train the model to learn the relationship y = 2x - 1.\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0df8588-cdba-4e8d-b9c9-65b2c233c205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:15:31.001603: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3ce0f352e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-27 20:15:31.001615: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-02-27 20:15:31.007598: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709082931.034221   46514 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3ecc197910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the dummy data.\n",
    "# We specify the number of iterations over the entire dataset (epochs) and suppress the training log (verbose=0).\n",
    "model.fit(xs, ys, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59079e8-b362-412d-b933-50e2118582a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaniganp/mambaforge/envs/tensorflow-exercise-0/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file in Hierarchical Data Format version 5 (HDF5).\n",
    "# This allows the model to be loaded and used later without retraining.\n",
    "model.save('../models/exercise_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2b2df7-e4e6-4a31-bb1d-e4fe558fc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the trained model into the TensorFlow Lite format.\n",
    "# This step prepares the model for deployment on mobile or embedded devices by reducing its size and potentially\n",
    "# improving inference speed.\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95167f88-9b6a-4825-b7ae-696573abe3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply default optimizations for the conversion process, including quantization.\n",
    "# Quantization reduces the precision of the model's weights and activations, which can decrease size and increase\n",
    "# inference speed with minimal impact on accuracy.\n",
    "converter.optimizations = [lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795a6554-8f3a-4a28-9e80-8a45689749ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8vieiiwn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8vieiiwn/assets\n",
      "2024-02-27 20:15:53.826014: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-27 20:15:53.826024: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-27 20:15:53.826157: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp8vieiiwn\n",
      "2024-02-27 20:15:53.826412: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-27 20:15:53.826415: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp8vieiiwn\n",
      "2024-02-27 20:15:53.827244: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-27 20:15:53.827419: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-27 20:15:53.837208: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp8vieiiwn\n",
      "2024-02-27 20:15:53.839743: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 13586 microseconds.\n",
      "2024-02-27 20:15:53.843787: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 2, Total Ops 6, % non-converted = 33.33 %\n",
      " * 2 ARITH ops\n",
      "\n",
      "- arith.constant:    2 occurrences  (f32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to its TensorFlow Lite version with applied optimizations.\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3efb09ed-be2d-4588-879d-41db80aa5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted (and possibly quantized) TensorFlow Lite model to a file.\n",
    "# The model is now ready to be deployed on compatible devices.\n",
    "with open('../models/exercise_0.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
