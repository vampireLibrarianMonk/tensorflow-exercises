{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdb3232-8f92-419c-b4e8-0bd7e52451ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice to have and only here as a reference until moved to its instructional home :)\n",
    "#export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn; print(nvidia.cudnn.__file__)\"))\n",
    "#export SITE_PACKAGES_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\n",
    "#export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$SITE_PACKAGES_PATH/tensorrt_libs/:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1087c02-063b-41d7-be49-6d2ec87eb26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/flaniganp/mambaforge/envs/tensorflow-exercise-0:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       2_gnu    conda-forge\n",
      "absl-py                   2.1.0                    pypi_0    pypi\n",
      "anyio                     4.3.0                    pypi_0    pypi\n",
      "argon2-cffi               23.1.0                   pypi_0    pypi\n",
      "argon2-cffi-bindings      21.2.0                   pypi_0    pypi\n",
      "arrow                     1.3.0                    pypi_0    pypi\n",
      "asttokens                 2.4.1                    pypi_0    pypi\n",
      "async-lru                 2.0.4                    pypi_0    pypi\n",
      "attrs                     23.2.0                   pypi_0    pypi\n",
      "babel                     2.14.0                   pypi_0    pypi\n",
      "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
      "bleach                    6.1.0                    pypi_0    pypi\n",
      "bzip2                     1.0.8                hd590300_5    conda-forge\n",
      "ca-certificates           2024.2.2             hbcca054_0    conda-forge\n",
      "certifi                   2024.2.2                 pypi_0    pypi\n",
      "cffi                      1.16.0                   pypi_0    pypi\n",
      "charset-normalizer        3.3.2                    pypi_0    pypi\n",
      "comm                      0.2.1                    pypi_0    pypi\n",
      "debugpy                   1.8.1                    pypi_0    pypi\n",
      "decorator                 5.1.1                    pypi_0    pypi\n",
      "defusedxml                0.7.1                    pypi_0    pypi\n",
      "exceptiongroup            1.2.0                    pypi_0    pypi\n",
      "executing                 2.0.1                    pypi_0    pypi\n",
      "fastjsonschema            2.19.1                   pypi_0    pypi\n",
      "fqdn                      1.5.1                    pypi_0    pypi\n",
      "gast                      0.5.4                    pypi_0    pypi\n",
      "google-auth               2.28.1                   pypi_0    pypi\n",
      "grpcio                    1.62.0                   pypi_0    pypi\n",
      "h11                       0.14.0                   pypi_0    pypi\n",
      "httpcore                  1.0.4                    pypi_0    pypi\n",
      "httpx                     0.27.0                   pypi_0    pypi\n",
      "idna                      3.6                      pypi_0    pypi\n",
      "ipykernel                 6.29.3                   pypi_0    pypi\n",
      "ipython                   8.22.1                   pypi_0    pypi\n",
      "isoduration               20.11.0                  pypi_0    pypi\n",
      "jedi                      0.19.1                   pypi_0    pypi\n",
      "jinja2                    3.1.3                    pypi_0    pypi\n",
      "json5                     0.9.17                   pypi_0    pypi\n",
      "jsonpointer               2.4                      pypi_0    pypi\n",
      "jsonschema                4.21.1                   pypi_0    pypi\n",
      "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
      "jupyter-client            8.6.0                    pypi_0    pypi\n",
      "jupyter-core              5.7.1                    pypi_0    pypi\n",
      "jupyter-events            0.9.0                    pypi_0    pypi\n",
      "jupyter-lsp               2.2.3                    pypi_0    pypi\n",
      "jupyter-server            2.12.5                   pypi_0    pypi\n",
      "jupyter-server-terminals  0.5.2                    pypi_0    pypi\n",
      "jupyterlab                4.1.2                    pypi_0    pypi\n",
      "jupyterlab-pygments       0.3.0                    pypi_0    pypi\n",
      "jupyterlab-server         2.25.3                   pypi_0    pypi\n",
      "keras                     2.15.0                   pypi_0    pypi\n",
      "ld_impl_linux-64          2.40                 h41732ed_0    conda-forge\n",
      "libffi                    3.4.2                h7f98852_5    conda-forge\n",
      "libgcc-ng                 13.2.0               h807b86a_5    conda-forge\n",
      "libgomp                   13.2.0               h807b86a_5    conda-forge\n",
      "libnsl                    2.0.1                hd590300_0    conda-forge\n",
      "libsqlite                 3.45.1               h2797004_0    conda-forge\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
      "libzlib                   1.2.13               hd590300_5    conda-forge\n",
      "matplotlib-inline         0.1.6                    pypi_0    pypi\n",
      "mistune                   3.0.2                    pypi_0    pypi\n",
      "ml-dtypes                 0.2.0                    pypi_0    pypi\n",
      "nbclient                  0.9.0                    pypi_0    pypi\n",
      "nbconvert                 7.16.1                   pypi_0    pypi\n",
      "nbformat                  5.9.2                    pypi_0    pypi\n",
      "ncurses                   6.4                  h59595ed_2    conda-forge\n",
      "nest-asyncio              1.6.0                    pypi_0    pypi\n",
      "notebook                  7.1.1                    pypi_0    pypi\n",
      "notebook-shim             0.2.4                    pypi_0    pypi\n",
      "numpy                     1.26.4                   pypi_0    pypi\n",
      "nvidia-cublas-cu12        12.2.5.6                 pypi_0    pypi\n",
      "nvidia-cuda-cupti-cu12    12.2.142                 pypi_0    pypi\n",
      "nvidia-cuda-nvcc-cu12     12.2.140                 pypi_0    pypi\n",
      "nvidia-cuda-nvrtc-cu12    12.2.140                 pypi_0    pypi\n",
      "nvidia-cuda-runtime-cu12  12.2.140                 pypi_0    pypi\n",
      "nvidia-cudnn-cu12         8.9.4.25                 pypi_0    pypi\n",
      "nvidia-cufft-cu12         11.0.8.103               pypi_0    pypi\n",
      "nvidia-curand-cu12        10.3.3.141               pypi_0    pypi\n",
      "nvidia-cusolver-cu12      11.5.2.141               pypi_0    pypi\n",
      "nvidia-cusparse-cu12      12.1.2.141               pypi_0    pypi\n",
      "nvidia-nccl-cu12          2.16.5                   pypi_0    pypi\n",
      "nvidia-nvjitlink-cu12     12.2.140                 pypi_0    pypi\n",
      "oauthlib                  3.2.2                    pypi_0    pypi\n",
      "openssl                   3.2.1                hd590300_0    conda-forge\n",
      "overrides                 7.7.0                    pypi_0    pypi\n",
      "pandocfilters             1.5.1                    pypi_0    pypi\n",
      "parso                     0.8.3                    pypi_0    pypi\n",
      "pexpect                   4.9.0                    pypi_0    pypi\n",
      "pip                       24.0               pyhd8ed1ab_0    conda-forge\n",
      "platformdirs              4.2.0                    pypi_0    pypi\n",
      "prometheus-client         0.20.0                   pypi_0    pypi\n",
      "prompt-toolkit            3.0.43                   pypi_0    pypi\n",
      "protobuf                  4.25.3                   pypi_0    pypi\n",
      "psutil                    5.9.8                    pypi_0    pypi\n",
      "ptyprocess                0.7.0                    pypi_0    pypi\n",
      "pure-eval                 0.2.2                    pypi_0    pypi\n",
      "pyasn1                    0.5.1                    pypi_0    pypi\n",
      "pycparser                 2.21                     pypi_0    pypi\n",
      "pygments                  2.17.2                   pypi_0    pypi\n",
      "python                    3.10.0          h543edf9_3_cpython    conda-forge\n",
      "python-dateutil           2.8.2                    pypi_0    pypi\n",
      "python-json-logger        2.0.7                    pypi_0    pypi\n",
      "pyyaml                    6.0.1                    pypi_0    pypi\n",
      "pyzmq                     25.1.2                   pypi_0    pypi\n",
      "readline                  8.2                  h8228510_1    conda-forge\n",
      "referencing               0.33.0                   pypi_0    pypi\n",
      "requests                  2.31.0                   pypi_0    pypi\n",
      "rfc3339-validator         0.1.4                    pypi_0    pypi\n",
      "rfc3986-validator         0.1.1                    pypi_0    pypi\n",
      "rpds-py                   0.18.0                   pypi_0    pypi\n",
      "send2trash                1.8.2                    pypi_0    pypi\n",
      "setuptools                69.1.1             pyhd8ed1ab_0    conda-forge\n",
      "six                       1.16.0                   pypi_0    pypi\n",
      "sniffio                   1.3.1                    pypi_0    pypi\n",
      "soupsieve                 2.5                      pypi_0    pypi\n",
      "sqlite                    3.45.1               h2c6b66d_0    conda-forge\n",
      "stack-data                0.6.3                    pypi_0    pypi\n",
      "tensorboard               2.15.2                   pypi_0    pypi\n",
      "tensorflow                2.15.0.post1             pypi_0    pypi\n",
      "tensorflow-estimator      2.15.0                   pypi_0    pypi\n",
      "terminado                 0.18.0                   pypi_0    pypi\n",
      "tinycss2                  1.2.1                    pypi_0    pypi\n",
      "tk                        8.6.13          noxft_h4845f30_101    conda-forge\n",
      "tomli                     2.0.1                    pypi_0    pypi\n",
      "tornado                   6.4                      pypi_0    pypi\n",
      "traitlets                 5.14.1                   pypi_0    pypi\n",
      "types-python-dateutil     2.8.19.20240106          pypi_0    pypi\n",
      "typing-extensions         4.10.0                   pypi_0    pypi\n",
      "tzdata                    2024a                h0c530f3_0    conda-forge\n",
      "uri-template              1.3.0                    pypi_0    pypi\n",
      "urllib3                   2.2.1                    pypi_0    pypi\n",
      "wcwidth                   0.2.13                   pypi_0    pypi\n",
      "webcolors                 1.13                     pypi_0    pypi\n",
      "webencodings              0.5.1                    pypi_0    pypi\n",
      "websocket-client          1.7.0                    pypi_0    pypi\n",
      "wheel                     0.42.0             pyhd8ed1ab_0    conda-forge\n",
      "wrapt                     1.14.1                   pypi_0    pypi\n",
      "xz                        5.2.6                h166bdaf_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd7f885-6786-495a-ad48-34743ea54ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:13:29.135115: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-27 21:13:29.152459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-27 21:13:29.152475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-27 21:13:29.152928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-27 21:13:29.155963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 21:13:29.477955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# The os module in Python provides a way of using operating system dependent functionality. It allows you to interface\n",
    "# with the underlying operating system that Python is running on â€“ be it Windows, Mac or Linux. You can use the os module\n",
    "# to handle file and directory paths, create folders, list contents of a directory, manage environment variables, execute\n",
    "# shell commands, and more.\n",
    "import os\n",
    "\n",
    "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and\n",
    "# matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow is an open-source machine learning library developed by Google. It's used for both research and production\n",
    "# at Google.\n",
    "# * keras: Originally an independent neural network library, now integrated into TensorFlow, simplifies the creation and\n",
    "#   training of deep learning models. Keras is known for its user-friendliness and modular approach, allowing for easy\n",
    "#   and fast prototyping. It provides high-level building blocks for developing deep learning models while still\n",
    "#   enabling users to dive into lower-level operations if needed.\n",
    "from tensorflow import keras\n",
    "# * tensorflow.python.client: Provides functionalities to query the properties of the hardware devices TensorFlow can\n",
    "#   access. Specifically, this module is often used to list and get detailed information about the system's available\n",
    "#   CPUs, GPUs, and other hardware accelerators compatible with TensorFlow.\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Versioning sourcing\n",
    "from tensorflow import __version__ as tf_version\n",
    "\n",
    "# TensorFlow Lite provides tools and classes for converting TensorFlow models into a highly optimized format suitable\n",
    "# for deployment on mobile devices, embedded systems, or other platforms with limited computational capacity. This\n",
    "# module includes functionalities for model conversion, optimization, and inference. By importing `lite`, you gain\n",
    "# access to the TFLiteConverter class for model conversion, optimization options like quantization, and utilities for\n",
    "# running TFLite models on target devices.\n",
    "from tensorflow import lite\n",
    "\n",
    "# Importing specific modules from keras, which is now part of TensorFlow\n",
    "# Callbacks are utilities called at certain points during model training. EarlyStopping stops training when a monitored\n",
    "# metric has stopped improving, and ModelCheckpoint saves the model after every epoch.\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# load_model is used to load a saved model. Sequential is a linear stack of layers.\n",
    "from keras.models import load_model, Sequential\n",
    "# Dense is a standard layer type that is used in many neural networks.\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Regular Expressions\n",
    "# 1. search: This function is used to perform a search for a pattern in a string and returns a match object if the\n",
    "# pattern is found, otherwise None. It's particularly useful for string pattern matching and extracting specific\n",
    "# segments from text.\n",
    "from re import search\n",
    "\n",
    "# Key aspects of 'check_output':\n",
    "# 1. **Process Execution**: The 'check_output' function is used to run a command in the subprocess/external process and\n",
    "#    capture its output. This is especially useful for running system commands and capturing their output directly\n",
    "#    within a Python script.\n",
    "# 2. **Return Output**: It returns the output of the command, making it available to the Python environment. If the\n",
    "#    called command results in an error (non-zero exit status), it raises a CalledProcessError.\n",
    "# 3. **Use Cases**: Common use cases include executing a shell command, reading the output of a command, automating\n",
    "#    scripts that interact with the command line, and integrating external tools into a Python workflow.\n",
    "# Example Usage:\n",
    "# Suppose you want to capture the output of the 'ls' command in a Unix/Linux system. You can use 'check_output' like\n",
    "# this:\n",
    "# output = check_output(['ls', '-l'])\n",
    "from subprocess import check_output\n",
    "# Key aspects of 'CalledProcessError':\n",
    "#  1. Error Handling: CalledProcessError is an exception raised by check_output when the command it tries to execute\n",
    "#   returns a non-zero exit status, indicating failure. This exception is particularly useful for error handling in\n",
    "#   scripts where the success of an external command is crucial.\n",
    "#  2. Exception Details: The exception object contains information about the error, including the return code, command\n",
    "#  executed, and output (if any). This aids in debugging by providing clear insights into why the external command\n",
    "#  failed.\n",
    "#  3. Handling the Exception: In practical use, it is often caught in a try-except block, allowing the script to respond\n",
    "#  appropriately to the failure of the external command, like logging the error or trying a fallback operation.\n",
    "from subprocess import CalledProcessError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788195d7-47fb-474a-ae47-36530c6ec7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertinent declarations\n",
    "offset = 7\n",
    "training_depth = 10000\n",
    "model_path='../../models/exercise_1.h5'\n",
    "quantized_model_path='../../models/exercise_1.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b81d01a-f27c-45df-9878-4f9185db3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_path, offset, training_depth):\n",
    "    # Creating a simple dataset\n",
    "    training_range = np.arange(-training_depth, training_depth)  # np.arange creates evenly spaced values within a given interval.\n",
    "\n",
    "    # This case is the simplest one I could find adding the offset to the training range to create a test range for\n",
    "    # later prediction.\n",
    "    test_range = training_range + offset  # Simple linear relationship for the target variable\n",
    "\n",
    "    # In the context of neural networks, data types are crucial for managing memory and computational efficiency.\n",
    "    # float32 is a common data type representing a 32-bit floating-point number.\n",
    "    # It's widely used in neural network computations for a balance between precision and memory usage.\n",
    "    digit = 'float32'\n",
    "\n",
    "    # Reshaping and converting data type for TensorFlow compatibility\n",
    "    # The -1 tells NumPy to calculate the size of this dimension automatically based on the length of the array and the\n",
    "    # other given dimension, which is 1. This effectively transforms the array into a two-dimensional array with one\n",
    "    # column and as many rows as necessary to accommodate all elements.\n",
    "    x_train = training_range.reshape(-1, 1).astype(digit)\n",
    "    y_train = test_range.reshape(-1, 1).astype(digit)\n",
    "\n",
    "    # Building the neural network model\n",
    "    # Dense layer with a single neuron. Input shape is 1 since our input has only one feature.\n",
    "    model_1 = Sequential([\n",
    "        Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "\n",
    "    # Setting up the early stopping callback\n",
    "    # Mean Absolute Error (MAE) is the average of the absolute differences between the predicted values and the actual\n",
    "    # values. It measures how close the predictions of a model are to the actual outcomes.\n",
    "    monitor_metric = 'mae'\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=monitor_metric,  # Monitor the mean absolute error\n",
    "        patience=5  # Number of epochs with no improvement after which training will be stopped.\n",
    "    )\n",
    "\n",
    "    # Setting up the model checkpoint callback\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=model_path,  # File path to save the model\n",
    "        save_best_only=True,  # Save only the model that has the best performance on the monitored metric\n",
    "        monitor=monitor_metric,  # Metric to monitor\n",
    "        mode='min'  # The training will aim to minimize the monitored metric\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model_1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                    loss=keras.losses.mean_squared_error,\n",
    "                    metrics=[monitor_metric])\n",
    "\n",
    "    # Training the model\n",
    "    model_1.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=35,  # The number of times to iterate over the training data arrays\n",
    "        batch_size=32,  # Number of samples per gradient update\n",
    "        callbacks=[\n",
    "            early_stopping_callback,  # Implementing early stopping\n",
    "            model_checkpoint_callback  # Implementing model checkpoint saving\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model_1  # Returning the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f5b094-4745-4c72-8d51-3b0e96589613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function `print_gpu_info` is designed to display detailed information about the available GPUs on the system.\n",
    "# It utilizes TensorFlow's `device_lib.list_local_devices()` method to enumerate all computing devices recognized by\n",
    "# TensorFlow. For each device identified as a GPU, the function extracts and prints relevant details including the GPU's\n",
    "# ID, name, memory limit (converted to megabytes), and compute capability. The extraction of GPU information involves\n",
    "# parsing the device's description string using regular expressions to find specific pieces of information. This\n",
    "# function can be particularly useful for debugging or for setting up configurations in environments with multiple GPUs,\n",
    "# ensuring that TensorFlow is utilizing the GPUs as expected.\n",
    "\n",
    "def print_gpu_info():\n",
    "    # Undocumented Method\n",
    "    # https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    # Get the list of all devices\n",
    "    devices = device_lib.list_local_devices()\n",
    "\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            # Extract the physical device description\n",
    "            desc = device.physical_device_desc\n",
    "\n",
    "            # Use regular expressions to extract the required information\n",
    "            gpu_id_match = search(r'device: (\\d+)', desc)\n",
    "            name_match = search(r'name: (.*?),', desc)\n",
    "            compute_capability_match = search(r'compute capability: (\\d+\\.\\d+)', desc)\n",
    "\n",
    "            if gpu_id_match and name_match and compute_capability_match:\n",
    "                gpu_id = gpu_id_match.group(1)\n",
    "                gpu_name = name_match.group(1)\n",
    "                compute_capability = compute_capability_match.group(1)\n",
    "\n",
    "                # Convert memory limit from bytes to gigabytes and round it\n",
    "                memory_limit_gb = round(device.memory_limit / (1024 ** 2))\n",
    "\n",
    "                print(\n",
    "                    f\"\\tGPU ID {gpu_id} --> {gpu_name} --> \"\n",
    "                    f\"Memory Limit {memory_limit_gb} MB --> \"\n",
    "                    f\"Compute Capability {compute_capability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afee4aa-a3c3-484e-ba87-8f2dfdb5b6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Found:\n",
      "\tGPU ID 0 --> NVIDIA GeForce RTX 3090 --> Memory Limit 22018 MB --> Compute Capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:13:29.806161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 22018 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Hardware\n",
    "print(\"Hardware Found:\")\n",
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18d7427-2c4e-42a5-ace9-f7f1d45c4590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA Driver: 545.23.08\n",
      "Maximum Supported CUDA Version: 12.3     \n"
     ]
    }
   ],
   "source": [
    "# NVIDIA Driver\n",
    "try:\n",
    "    # Execute the nvidia-smi command and decode the output\n",
    "    nvidia_smi_output = check_output(\"nvidia-smi\", shell=True).decode()\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = nvidia_smi_output.split('\\n')\n",
    "\n",
    "    # Find the line containing the driver version\n",
    "    driver_line = next((line for line in lines if \"Driver Version\" in line), None)\n",
    "\n",
    "    # Extract the driver version number\n",
    "    if driver_line:\n",
    "        driver_version = driver_line.split('Driver Version: ')[1].split()[0]\n",
    "        print(\"NVIDIA Driver:\", driver_version)\n",
    "\n",
    "        # Extract the maximum supported CUDA version\n",
    "        cuda_version = driver_line.split('CUDA Version: ')[1].strip().replace(\"|\", \"\")\n",
    "        print(\"Maximum Supported CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"NVIDIA Driver Version or CUDA Version not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching NVIDIA Driver Version or CUDA Version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee624f0c-945a-4d38-a2b9-62b514d2317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Versions:\n",
      "CUDA Version 11.8.89\n"
     ]
    }
   ],
   "source": [
    "print(\"Software Versions:\")\n",
    "\n",
    "# CUDA\n",
    "try:\n",
    "    # Execute the 'nvcc --version' command and decode the output\n",
    "    nvcc_output = check_output(\"nvcc --version\", shell=True).decode()\n",
    "\n",
    "    # Use regular expression to find the version number\n",
    "    match = search(r\"V(\\d+\\.\\d+\\.\\d+)\", nvcc_output)\n",
    "    if match:\n",
    "        cuda_version = match.group(1)\n",
    "        print(\"CUDA Version\", cuda_version)\n",
    "    else:\n",
    "        print(\"CUDA Version not found\")\n",
    "\n",
    "except CalledProcessError as e:\n",
    "    print(\"Error executing nvcc --version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f3af25-16bc-4518-9170-b779896091bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:13:29.883823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22018 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "225/625 [=========>....................] - ETA: 0s - loss: 75878.4297 - mae: 89.3965    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:13:30.356513: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f0f114032d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-27 21:13:30.356525: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-02-27 21:13:30.358908: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-27 21:13:30.366769: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709086410.405691   58933 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 450us/step - loss: 27341.9199 - mae: 36.2295\n",
      "Epoch 2/35\n",
      "353/625 [===============>..............] - ETA: 0s - loss: 28.6168 - mae: 5.3155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaniganp/mambaforge/envs/tensorflow-exercise-0/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 433us/step - loss: 24.5894 - mae: 4.9056\n",
      "Epoch 3/35\n",
      "625/625 [==============================] - 0s 426us/step - loss: 9.7200 - mae: 3.0579\n",
      "Epoch 4/35\n",
      "625/625 [==============================] - 0s 423us/step - loss: 2.5781 - mae: 1.5435\n",
      "Epoch 5/35\n",
      "625/625 [==============================] - 0s 415us/step - loss: 0.4020 - mae: 0.5932\n",
      "Epoch 6/35\n",
      "625/625 [==============================] - 0s 444us/step - loss: 0.0314 - mae: 0.1579\n",
      "Epoch 7/35\n",
      "625/625 [==============================] - 0s 452us/step - loss: 9.3798e-04 - mae: 0.0255\n",
      "Epoch 8/35\n",
      "625/625 [==============================] - 0s 420us/step - loss: 8.1599e-06 - mae: 0.0021\n",
      "Epoch 9/35\n",
      "625/625 [==============================] - 0s 381us/step - loss: 1.3539e-08 - mae: 5.6767e-05\n",
      "Epoch 10/35\n",
      "625/625 [==============================] - 0s 372us/step - loss: 2.8145e-09 - mae: 2.3853e-05\n",
      "Epoch 11/35\n",
      "625/625 [==============================] - 0s 405us/step - loss: 1.2563e-09 - mae: 1.3278e-05\n",
      "Epoch 12/35\n",
      "625/625 [==============================] - 0s 400us/step - loss: 3.8147e-10 - mae: 6.2500e-06\n",
      "Epoch 13/35\n",
      "625/625 [==============================] - 0s 413us/step - loss: 3.8147e-10 - mae: 6.2500e-06\n",
      "Epoch 14/35\n",
      "625/625 [==============================] - 0s 410us/step - loss: 3.7627e-10 - mae: 6.2032e-06\n",
      "Epoch 15/35\n",
      "625/625 [==============================] - 0s 412us/step - loss: 1.9151e-10 - mae: 3.8560e-06\n",
      "Epoch 16/35\n",
      "625/625 [==============================] - 0s 387us/step - loss: 4.7684e-11 - mae: 1.5625e-06\n",
      "Epoch 17/35\n",
      "625/625 [==============================] - 0s 406us/step - loss: 4.7684e-11 - mae: 1.5625e-06\n",
      "Epoch 18/35\n",
      "625/625 [==============================] - 0s 385us/step - loss: 4.7085e-11 - mae: 1.5518e-06\n",
      "Epoch 19/35\n",
      "625/625 [==============================] - 0s 388us/step - loss: 2.9582e-11 - mae: 1.1388e-06\n",
      "Epoch 20/35\n",
      "625/625 [==============================] - 0s 405us/step - loss: 5.9605e-12 - mae: 3.9062e-07\n",
      "Epoch 21/35\n",
      "625/625 [==============================] - 0s 398us/step - loss: 5.9605e-12 - mae: 3.9062e-07\n",
      "Epoch 22/35\n",
      "625/625 [==============================] - 0s 411us/step - loss: 5.6671e-12 - mae: 3.7975e-07\n",
      "Epoch 23/35\n",
      "625/625 [==============================] - 0s 540us/step - loss: 3.5321e-12 - mae: 2.7742e-07\n",
      "Epoch 24/35\n",
      "625/625 [==============================] - 0s 496us/step - loss: 7.4506e-13 - mae: 9.7656e-08\n",
      "Epoch 25/35\n",
      "625/625 [==============================] - 1s 914us/step - loss: 7.4506e-13 - mae: 9.7656e-08\n",
      "Epoch 26/35\n",
      "625/625 [==============================] - 0s 433us/step - loss: 6.8205e-13 - mae: 9.2363e-08\n",
      "Epoch 27/35\n",
      "625/625 [==============================] - 0s 430us/step - loss: 1.0536e-13 - mae: 2.5988e-08\n",
      "Epoch 28/35\n",
      "625/625 [==============================] - 0s 459us/step - loss: 9.0404e-14 - mae: 2.4033e-08\n",
      "Epoch 29/35\n",
      "625/625 [==============================] - 0s 404us/step - loss: 9.0404e-14 - mae: 2.4033e-08\n",
      "Epoch 30/35\n",
      "625/625 [==============================] - 0s 472us/step - loss: 6.8849e-14 - mae: 2.0266e-08\n",
      "Epoch 31/35\n",
      "625/625 [==============================] - 0s 476us/step - loss: 1.5530e-14 - mae: 5.5790e-09\n",
      "Epoch 32/35\n",
      "625/625 [==============================] - 0s 480us/step - loss: 1.4552e-15 - mae: 1.5259e-09\n",
      "Epoch 33/35\n",
      "625/625 [==============================] - 0s 703us/step - loss: 1.4552e-15 - mae: 1.5259e-09\n",
      "Epoch 34/35\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 1.4552e-15 - mae: 1.5259e-09\n",
      "Epoch 35/35\n",
      "625/625 [==============================] - 0s 529us/step - loss: 1.4552e-15 - mae: 1.5259e-09\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = create_model(model_path, offset, training_depth)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adeee3eb-f5eb-4411-8d30-ad286811a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best saved model\n",
    "saved_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8694a466-0508-4c3a-94e7-361427c60e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Depth = 100 with offset of 7\n",
      "Predicted -93.0 for -100\n",
      "Predicted -83.0 for -90\n",
      "Predicted -73.0 for -80\n",
      "Predicted -63.0 for -70\n",
      "Predicted -53.0 for -60\n",
      "Predicted -43.0 for -50\n",
      "Predicted -33.0 for -40\n",
      "Predicted -23.0 for -30\n",
      "Predicted -13.000000953674316 for -20\n",
      "Predicted -3.0000009536743164 for -10\n",
      "Predicted 6.999999046325684 for 0\n",
      "Predicted 17.0 for 10\n",
      "Predicted 27.0 for 20\n",
      "Predicted 37.0 for 30\n",
      "Predicted 47.0 for 40\n",
      "Predicted 57.0 for 50\n",
      "Predicted 67.0 for 60\n",
      "Predicted 77.0 for 70\n",
      "Predicted 87.0 for 80\n",
      "Predicted 97.0 for 90\n",
      "Predicted 107.0 for 100\n"
     ]
    }
   ],
   "source": [
    "# Using the model for prediction\n",
    "predicted_depth = 100\n",
    "base_x = np.arange(-predicted_depth, predicted_depth + 1, 10)  # New data for prediction\n",
    "new_x_values = base_x.reshape(-1, 1)  # Reshaping data for prediction\n",
    "predicted_y = saved_model.predict(new_x_values)  # Making predictions\n",
    "\n",
    "# Show the new dataset and the associated predictions\n",
    "print(f\"Predicted Depth = {predicted_depth} with offset of {offset}\")\n",
    "for predicted_index in range(0, len(predicted_y.flatten())):\n",
    "    print(f\"Predicted {predicted_y.flatten()[predicted_index]} for {new_x_values.flatten()[predicted_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35321e8a-1c1f-42fb-8930-1b73fea99a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 20240 bytes, or 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a848d01-a284-44c7-bcb1-4bf0ae1fd216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx367194k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx367194k/assets\n",
      "2024-02-27 21:13:41.016456: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-02-27 21:13:41.016467: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-02-27 21:13:41.016649: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpx367194k\n",
      "2024-02-27 21:13:41.016918: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-02-27 21:13:41.016922: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpx367194k\n",
      "2024-02-27 21:13:41.017822: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-02-27 21:13:41.018033: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-02-27 21:13:41.030240: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpx367194k\n",
      "2024-02-27 21:13:41.033245: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 16597 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 2, Total Ops 6, % non-converted = 33.33 %\n",
      " * 2 ARITH ops\n",
      "\n",
      "- arith.constant:    2 occurrences  (f32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [lite.Optimize.DEFAULT]\n",
    "\n",
    "# Generate a quantized model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open(quantized_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca778cf-3a56-43d1-961d-47e461449926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model size: 1084 bytes, or 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Quantized Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facf86b4-bef0-4f33-a4f1-b261272e85f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Predicted Depth = 100 with offset of 7\n",
      "Quantized Predicted -93.0 for -100.0\n",
      "Quantized Predicted -83.0 for -90.0\n",
      "Quantized Predicted -73.0 for -80.0\n",
      "Quantized Predicted -63.0 for -70.0\n",
      "Quantized Predicted -53.0 for -60.0\n",
      "Quantized Predicted -43.0 for -50.0\n",
      "Quantized Predicted -33.0 for -40.0\n",
      "Quantized Predicted -23.0 for -30.0\n",
      "Quantized Predicted -13.000000953674316 for -20.0\n",
      "Quantized Predicted -3.0000009536743164 for -10.0\n",
      "Quantized Predicted 6.999999046325684 for 0.0\n",
      "Quantized Predicted 17.0 for 10.0\n",
      "Quantized Predicted 27.0 for 20.0\n",
      "Quantized Predicted 37.0 for 30.0\n",
      "Quantized Predicted 47.0 for 40.0\n",
      "Quantized Predicted 57.0 for 50.0\n",
      "Quantized Predicted 67.0 for 60.0\n",
      "Quantized Predicted 77.0 for 70.0\n",
      "Quantized Predicted 87.0 for 80.0\n",
      "Quantized Predicted 97.0 for 90.0\n",
      "Quantized Predicted 107.0 for 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TensorFlow Lite interpreter\n",
    "interpreter = lite.Interpreter(model_path=quantized_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input and output details for the interpreter\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Assuming `predicted_depth` and `offset` are defined as in your prompt\n",
    "predicted_depth = 100\n",
    "offset = 7  \n",
    "\n",
    "# Generate new data for prediction\n",
    "base_x = np.arange(-predicted_depth, predicted_depth + 1, 10).reshape(-1, 1)  # New data for prediction\n",
    "\n",
    "# Ensure the input data type matches the model's expected input type\n",
    "base_x = base_x.astype(input_details[0]['dtype'])\n",
    "\n",
    "# Show the new dataset and the associated predictions\n",
    "print(f\"Quantized Predicted Depth = {predicted_depth} with offset of {offset}\")\n",
    "\n",
    "for i in range(base_x.shape[0]):\n",
    "    # Set the tensor to the input index 0 (as most models have a single input)\n",
    "    interpreter.set_tensor(input_details[0]['index'], base_x[i:i+1])\n",
    "    \n",
    "    # Run the interpreter\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve the prediction from the output tensor at index 0\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Print the prediction along with the corresponding input\n",
    "    print(f\"Quantized Predicted {output_data[0][0]} for {base_x[i][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
