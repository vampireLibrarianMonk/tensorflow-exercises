{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bfdedd-09ed-4812-a6fd-172c53f7152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice to have and only here as a reference until moved to its instructional home :)\n",
    "#export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn; print(nvidia.cudnn.__file__)\"))\n",
    "#export SITE_PACKAGES_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\n",
    "#export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$SITE_PACKAGES_PATH/tensorrt_libs/:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79de22b2-15ab-4fc4-9bab-08cdd10a25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/flaniganp/mambaforge/envs/tensorflow-exercise-2:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       2_gnu    conda-forge\n",
      "absl-py                   2.1.0                    pypi_0    pypi\n",
      "anyio                     4.3.0                    pypi_0    pypi\n",
      "argon2-cffi               23.1.0                   pypi_0    pypi\n",
      "argon2-cffi-bindings      21.2.0                   pypi_0    pypi\n",
      "arrow                     1.3.0                    pypi_0    pypi\n",
      "asttokens                 2.4.1                    pypi_0    pypi\n",
      "async-lru                 2.0.4                    pypi_0    pypi\n",
      "attrs                     23.2.0                   pypi_0    pypi\n",
      "babel                     2.14.0                   pypi_0    pypi\n",
      "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
      "bleach                    6.1.0                    pypi_0    pypi\n",
      "bzip2                     1.0.8                hd590300_5    conda-forge\n",
      "ca-certificates           2024.2.2             hbcca054_0    conda-forge\n",
      "certifi                   2024.2.2                 pypi_0    pypi\n",
      "cffi                      1.16.0                   pypi_0    pypi\n",
      "charset-normalizer        3.3.2                    pypi_0    pypi\n",
      "comm                      0.2.1                    pypi_0    pypi\n",
      "contourpy                 1.2.0                    pypi_0    pypi\n",
      "cycler                    0.12.1                   pypi_0    pypi\n",
      "debugpy                   1.8.1                    pypi_0    pypi\n",
      "decorator                 5.1.1                    pypi_0    pypi\n",
      "defusedxml                0.7.1                    pypi_0    pypi\n",
      "exceptiongroup            1.2.0                    pypi_0    pypi\n",
      "executing                 2.0.1                    pypi_0    pypi\n",
      "fastjsonschema            2.19.1                   pypi_0    pypi\n",
      "fonttools                 4.49.0                   pypi_0    pypi\n",
      "fqdn                      1.5.1                    pypi_0    pypi\n",
      "gast                      0.5.4                    pypi_0    pypi\n",
      "google-auth               2.28.1                   pypi_0    pypi\n",
      "grpcio                    1.62.0                   pypi_0    pypi\n",
      "h11                       0.14.0                   pypi_0    pypi\n",
      "httpcore                  1.0.4                    pypi_0    pypi\n",
      "httpx                     0.27.0                   pypi_0    pypi\n",
      "idna                      3.6                      pypi_0    pypi\n",
      "ipykernel                 6.29.3                   pypi_0    pypi\n",
      "ipython                   8.22.1                   pypi_0    pypi\n",
      "ipywidgets                8.1.2                    pypi_0    pypi\n",
      "isoduration               20.11.0                  pypi_0    pypi\n",
      "jedi                      0.19.1                   pypi_0    pypi\n",
      "jinja2                    3.1.3                    pypi_0    pypi\n",
      "json5                     0.9.17                   pypi_0    pypi\n",
      "jsonpointer               2.4                      pypi_0    pypi\n",
      "jsonschema                4.21.1                   pypi_0    pypi\n",
      "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
      "jupyter                   1.0.0                    pypi_0    pypi\n",
      "jupyter-client            8.6.0                    pypi_0    pypi\n",
      "jupyter-console           6.6.3                    pypi_0    pypi\n",
      "jupyter-core              5.7.1                    pypi_0    pypi\n",
      "jupyter-events            0.9.0                    pypi_0    pypi\n",
      "jupyter-lsp               2.2.3                    pypi_0    pypi\n",
      "jupyter-server            2.12.5                   pypi_0    pypi\n",
      "jupyter-server-terminals  0.5.2                    pypi_0    pypi\n",
      "jupyterlab                4.1.2                    pypi_0    pypi\n",
      "jupyterlab-pygments       0.3.0                    pypi_0    pypi\n",
      "jupyterlab-server         2.25.3                   pypi_0    pypi\n",
      "jupyterlab-widgets        3.0.10                   pypi_0    pypi\n",
      "keras                     2.15.0                   pypi_0    pypi\n",
      "kiwisolver                1.4.5                    pypi_0    pypi\n",
      "ld_impl_linux-64          2.40                 h41732ed_0    conda-forge\n",
      "libffi                    3.4.2                h7f98852_5    conda-forge\n",
      "libgcc-ng                 13.2.0               h807b86a_5    conda-forge\n",
      "libgomp                   13.2.0               h807b86a_5    conda-forge\n",
      "libnsl                    2.0.1                hd590300_0    conda-forge\n",
      "libsqlite                 3.45.1               h2797004_0    conda-forge\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
      "libzlib                   1.2.13               hd590300_5    conda-forge\n",
      "matplotlib                3.8.3                    pypi_0    pypi\n",
      "matplotlib-inline         0.1.6                    pypi_0    pypi\n",
      "mistune                   3.0.2                    pypi_0    pypi\n",
      "ml-dtypes                 0.2.0                    pypi_0    pypi\n",
      "nbclient                  0.9.0                    pypi_0    pypi\n",
      "nbconvert                 7.16.1                   pypi_0    pypi\n",
      "nbformat                  5.9.2                    pypi_0    pypi\n",
      "ncurses                   6.4                  h59595ed_2    conda-forge\n",
      "nest-asyncio              1.6.0                    pypi_0    pypi\n",
      "notebook                  7.1.1                    pypi_0    pypi\n",
      "notebook-shim             0.2.4                    pypi_0    pypi\n",
      "numpy                     1.26.4                   pypi_0    pypi\n",
      "nvidia-cublas-cu12        12.2.5.6                 pypi_0    pypi\n",
      "nvidia-cuda-cupti-cu12    12.2.142                 pypi_0    pypi\n",
      "nvidia-cuda-nvcc-cu12     12.2.140                 pypi_0    pypi\n",
      "nvidia-cuda-nvrtc-cu12    12.2.140                 pypi_0    pypi\n",
      "nvidia-cuda-runtime-cu12  12.2.140                 pypi_0    pypi\n",
      "nvidia-cudnn-cu12         8.9.4.25                 pypi_0    pypi\n",
      "nvidia-cufft-cu12         11.0.8.103               pypi_0    pypi\n",
      "nvidia-curand-cu12        10.3.3.141               pypi_0    pypi\n",
      "nvidia-cusolver-cu12      11.5.2.141               pypi_0    pypi\n",
      "nvidia-cusparse-cu12      12.1.2.141               pypi_0    pypi\n",
      "nvidia-nccl-cu12          2.16.5                   pypi_0    pypi\n",
      "nvidia-nvjitlink-cu12     12.2.140                 pypi_0    pypi\n",
      "oauthlib                  3.2.2                    pypi_0    pypi\n",
      "openssl                   3.2.1                hd590300_0    conda-forge\n",
      "overrides                 7.7.0                    pypi_0    pypi\n",
      "pandocfilters             1.5.1                    pypi_0    pypi\n",
      "parso                     0.8.3                    pypi_0    pypi\n",
      "pexpect                   4.9.0                    pypi_0    pypi\n",
      "pillow                    10.2.0                   pypi_0    pypi\n",
      "pip                       24.0               pyhd8ed1ab_0    conda-forge\n",
      "platformdirs              4.2.0                    pypi_0    pypi\n",
      "prometheus-client         0.20.0                   pypi_0    pypi\n",
      "prompt-toolkit            3.0.43                   pypi_0    pypi\n",
      "protobuf                  4.25.3                   pypi_0    pypi\n",
      "psutil                    5.9.8                    pypi_0    pypi\n",
      "ptyprocess                0.7.0                    pypi_0    pypi\n",
      "pure-eval                 0.2.2                    pypi_0    pypi\n",
      "pyasn1                    0.5.1                    pypi_0    pypi\n",
      "pycparser                 2.21                     pypi_0    pypi\n",
      "pygments                  2.17.2                   pypi_0    pypi\n",
      "pyparsing                 3.1.1                    pypi_0    pypi\n",
      "python                    3.10.0          h543edf9_3_cpython    conda-forge\n",
      "python-dateutil           2.8.2                    pypi_0    pypi\n",
      "python-json-logger        2.0.7                    pypi_0    pypi\n",
      "pyyaml                    6.0.1                    pypi_0    pypi\n",
      "pyzmq                     25.1.2                   pypi_0    pypi\n",
      "qtconsole                 5.5.1                    pypi_0    pypi\n",
      "qtpy                      2.4.1                    pypi_0    pypi\n",
      "readline                  8.2                  h8228510_1    conda-forge\n",
      "referencing               0.33.0                   pypi_0    pypi\n",
      "requests                  2.31.0                   pypi_0    pypi\n",
      "rfc3339-validator         0.1.4                    pypi_0    pypi\n",
      "rfc3986-validator         0.1.1                    pypi_0    pypi\n",
      "rpds-py                   0.18.0                   pypi_0    pypi\n",
      "send2trash                1.8.2                    pypi_0    pypi\n",
      "setuptools                69.1.1             pyhd8ed1ab_0    conda-forge\n",
      "six                       1.16.0                   pypi_0    pypi\n",
      "sniffio                   1.3.1                    pypi_0    pypi\n",
      "soupsieve                 2.5                      pypi_0    pypi\n",
      "sqlite                    3.45.1               h2c6b66d_0    conda-forge\n",
      "stack-data                0.6.3                    pypi_0    pypi\n",
      "tensorboard               2.15.2                   pypi_0    pypi\n",
      "tensorflow                2.15.0.post1             pypi_0    pypi\n",
      "tensorflow-estimator      2.15.0                   pypi_0    pypi\n",
      "terminado                 0.18.0                   pypi_0    pypi\n",
      "tinycss2                  1.2.1                    pypi_0    pypi\n",
      "tk                        8.6.13          noxft_h4845f30_101    conda-forge\n",
      "tomli                     2.0.1                    pypi_0    pypi\n",
      "tornado                   6.4                      pypi_0    pypi\n",
      "traitlets                 5.14.1                   pypi_0    pypi\n",
      "types-python-dateutil     2.8.19.20240106          pypi_0    pypi\n",
      "typing-extensions         4.10.0                   pypi_0    pypi\n",
      "tzdata                    2024a                h0c530f3_0    conda-forge\n",
      "uri-template              1.3.0                    pypi_0    pypi\n",
      "urllib3                   2.2.1                    pypi_0    pypi\n",
      "wcwidth                   0.2.13                   pypi_0    pypi\n",
      "webcolors                 1.13                     pypi_0    pypi\n",
      "webencodings              0.5.1                    pypi_0    pypi\n",
      "websocket-client          1.7.0                    pypi_0    pypi\n",
      "wheel                     0.42.0             pyhd8ed1ab_0    conda-forge\n",
      "widgetsnbextension        4.0.10                   pypi_0    pypi\n",
      "wrapt                     1.14.1                   pypi_0    pypi\n",
      "xz                        5.2.6                h166bdaf_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb7e7404-6470-414d-9cf4-6a8c9e3987b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * json: Is essential for reading and decoding JSON data from a file-like object, converting it into Python data\n",
    "# structures like dictionaries or lists. Renaming it to json_load can help in avoiding naming conflicts or provide a\n",
    "# more descriptive function name in the context of the code.\n",
    "from json import load as json_load\n",
    "# * keras.layers: This component of Keras provides a wide array of layers for building neural networks, including\n",
    "#   convolutional layers, pooling layers, dense (fully connected) layers, and more. These layers are the building blocks\n",
    "#   of neural networks and can be stacked to create complex architectures tailored to specific machine learning tasks.\n",
    "\n",
    "# Tensorflow Keras\n",
    "# * keras.layers: This component of Keras provides a wide array of layers for building neural networks, including\n",
    "#   convolutional layers, pooling layers, dense (fully connected) layers, and more. These layers are the building blocks\n",
    "#   of neural networks and can be stacked to create complex architectures tailored to specific machine learning tasks.\n",
    "from keras import layers\n",
    "# * keras.models: This module in Keras is essential for creating neural network models. It includes classes like\n",
    "#   Sequential and the Functional API for building models. The Sequential model is straightforward, allowing layers to\n",
    "#   be added in sequence, suitable for simple architectures. The Functional API, on the other hand, provides greater\n",
    "#   flexibility for creating complex models with advanced features like shared layers and multiple inputs/outputs.\n",
    "#   Both types enable comprehensive model management, including training, evaluation, and saving/loading\n",
    "#   functionalities, making them versatile for a wide range of deep learning applications.\n",
    "from keras import models\n",
    "# * keras.preprocessing.text: Is instrumental in text preprocessing for deep learning models, as it allows for the\n",
    "# conversion of text data into a more manageable form, typically sequences of integers, where each integer maps to a\n",
    "# specific word in the text. This conversion is essential for preparing textual data for training neural network models,\n",
    "# particularly in natural language processing tasks.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# * keras.preprocessing.sequence: Is widely used in preparing sequential data, particularly for neural network models in\n",
    "# natural language processing. It standardizes the lengths of sequences by padding them with zeros or truncating them to\n",
    "# a specified length, ensuring that all sequences in a dataset have the same length for batch processing.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# The os module in Python provides a way of using operating system dependent functionality. It allows you to interface\n",
    "# with the underlying operating system that Python is running on â€“ be it Windows, Mac or Linux. You can use the os module\n",
    "# to handle file and directory paths, create folders, list contents of a directory, manage environment variables, execute\n",
    "# shell commands, and more.\n",
    "import os\n",
    "\n",
    "# The statement `import numpy as np` is a convenient way to access all the powerful features of the NumPy library using \n",
    "# the shorthand `np`. NumPy is essential for scientific computing in Python, providing support for large, multi-dimensional\n",
    "# arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. \n",
    "# Using `np` as an abbreviation allows for more concise code, especially when dealing with complex mathematical operations \n",
    "# and data manipulation tasks.\n",
    "import numpy as np\n",
    "\n",
    "# Provides functions for generating random numbers and performing random operations, such as shuffling a list or choosing a\n",
    "# random element from a sequence. This module is useful for tasks that require randomness, like simulating experiments, \n",
    "# generating test data, or adding unpredictability to games and applications.\n",
    "import random\n",
    "\n",
    "# Regular Expressions\n",
    "# 1. search: This function is used to perform a search for a pattern in a string and returns a match object if the\n",
    "# pattern is found, otherwise None. It's particularly useful for string pattern matching and extracting specific\n",
    "# segments from text.\n",
    "from re import search\n",
    "\n",
    "# Key aspects of 'check_output':\n",
    "# 1. **Process Execution**: The 'check_output' function is used to run a command in the subprocess/external process and\n",
    "#    capture its output. This is especially useful for running system commands and capturing their output directly\n",
    "#    within a Python script.\n",
    "# 2. **Return Output**: It returns the output of the command, making it available to the Python environment. If the\n",
    "#    called command results in an error (non-zero exit status), it raises a CalledProcessError.\n",
    "# 3. **Use Cases**: Common use cases include executing a shell command, reading the output of a command, automating\n",
    "#    scripts that interact with the command line, and integrating external tools into a Python workflow.\n",
    "# Example Usage:\n",
    "# Suppose you want to capture the output of the 'ls' command in a Unix/Linux system. You can use 'check_output' like\n",
    "# this:\n",
    "# output = check_output(['ls', '-l'])\n",
    "from subprocess import check_output\n",
    "# Key aspects of 'CalledProcessError':\n",
    "#  1. Error Handling: CalledProcessError is an exception raised by check_output when the command it tries to execute\n",
    "#   returns a non-zero exit status, indicating failure. This exception is particularly useful for error handling in\n",
    "#   scripts where the success of an external command is crucial.\n",
    "#  2. Exception Details: The exception object contains information about the error, including the return code, command\n",
    "#  executed, and output (if any). This aids in debugging by providing clear insights into why the external command\n",
    "#  failed.\n",
    "#  3. Handling the Exception: In practical use, it is often caught in a try-except block, allowing the script to respond\n",
    "#  appropriately to the failure of the external command, like logging the error or trying a fallback operation.\n",
    "from subprocess import CalledProcessError\n",
    "\n",
    "# * tensorflow.python.client: Provides functionalities to query the properties of the hardware devices TensorFlow can\n",
    "#   access. Specifically, this module is often used to list and get detailed information about the system's available\n",
    "#   CPUs, GPUs, and other hardware accelerators compatible with TensorFlow.\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# TensorFlow Lite provides tools and classes for converting TensorFlow models into a highly optimized format suitable\n",
    "# for deployment on mobile devices, embedded systems, or other platforms with limited computational capacity. This\n",
    "# module includes functionalities for model conversion, optimization, and inference. By importing `lite`, you gain\n",
    "# access to the TFLiteConverter class for model conversion, optimization options like quantization, and utilities for\n",
    "# running TFLite models on target devices.\n",
    "from tensorflow import lite\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "# Versioning sourcing\n",
    "from tensorflow import __version__ as tf_version\n",
    "\n",
    "# The `urllib` package is a Python module used for opening and reading URLs. It offers a range of functionality for \n",
    "# interacting with URLs, particularly for web scraping, fetching data across the web, and handling URL components. \n",
    "# It provides support for various network protocols such as HTTP and FTP, making it a versatile tool for network programming.\n",
    "from urllib import request as url_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647e059e-7551-427c-9bc9-d2b865271380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow. For each device identified as a GPU, the function extracts and prints relevant details including the GPU's\n",
    "# ID, name, memory limit (converted to megabytes), and compute capability. The extraction of GPU information involves\n",
    "# parsing the device's description string using regular expressions to find specific pieces of information. This\n",
    "# function can be particularly useful for debugging or for setting up configurations in environments with multiple GPUs,\n",
    "# ensuring that TensorFlow is utilizing the GPUs as expected.\n",
    "\n",
    "def print_gpu_info():\n",
    "    # Undocumented Method\n",
    "    # https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    # Get the list of all devices\n",
    "    devices = device_lib.list_local_devices()\n",
    "\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            # Extract the physical device description\n",
    "            desc = device.physical_device_desc\n",
    "\n",
    "            # Use regular expressions to extract the required information\n",
    "            gpu_id_match = search(r'device: (\\d+)', desc)\n",
    "            name_match = search(r'name: (.*?),', desc)\n",
    "            compute_capability_match = search(r'compute capability: (\\d+\\.\\d+)', desc)\n",
    "\n",
    "            if gpu_id_match and name_match and compute_capability_match:\n",
    "                gpu_id = gpu_id_match.group(1)\n",
    "                gpu_name = name_match.group(1)\n",
    "                compute_capability = compute_capability_match.group(1)\n",
    "\n",
    "                # Convert memory limit from bytes to gigabytes and round it\n",
    "                memory_limit_gb = round(device.memory_limit / (1024 ** 2))\n",
    "\n",
    "                print(\n",
    "                    f\"\\tGPU ID {gpu_id} --> {gpu_name} --> \"\n",
    "                    f\"Memory Limit {memory_limit_gb} MB --> \"\n",
    "                    f\"Compute Capability {compute_capability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c0f1ab-efb0-46dd-9b1d-319238d5bbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA Driver: 545.23.08\n",
      "Maximum Supported CUDA Version: 12.3     \n"
     ]
    }
   ],
   "source": [
    "# NVIDIA Driver\n",
    "try:\n",
    "    # Execute the nvidia-smi command and decode the output\n",
    "    nvidia_smi_output = check_output(\"nvidia-smi\", shell=True).decode()\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = nvidia_smi_output.split('\\n')\n",
    "\n",
    "    # Find the line containing the driver version\n",
    "    driver_line = next((line for line in lines if \"Driver Version\" in line), None)\n",
    "\n",
    "    # Extract the driver version number\n",
    "    if driver_line:\n",
    "        driver_version = driver_line.split('Driver Version: ')[1].split()[0]\n",
    "        print(\"NVIDIA Driver:\", driver_version)\n",
    "\n",
    "        # Extract the maximum supported CUDA version\n",
    "        cuda_version = driver_line.split('CUDA Version: ')[1].strip().replace(\"|\", \"\")\n",
    "        print(\"Maximum Supported CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"NVIDIA Driver Version or CUDA Version not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching NVIDIA Driver Version or CUDA Version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bda2a7-1d9e-48ce-ba57-34a77f2eff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Versions:\n",
      "CUDA Version 11.8.89\n"
     ]
    }
   ],
   "source": [
    "print(\"Software Versions:\")\n",
    "\n",
    "# CUDA\n",
    "try:\n",
    "    # Execute the 'nvcc --version' command and decode the output\n",
    "    nvcc_output = check_output(\"nvcc --version\", shell=True).decode()\n",
    "\n",
    "    # Use regular expression to find the version number\n",
    "    match = search(r\"V(\\d+\\.\\d+\\.\\d+)\", nvcc_output)\n",
    "    if match:\n",
    "        cuda_version = match.group(1)\n",
    "        print(\"CUDA Version\", cuda_version)\n",
    "    else:\n",
    "        print(\"CUDA Version not found\")\n",
    "\n",
    "except CalledProcessError as e:\n",
    "    print(\"Error executing nvcc --version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6294e736-cbc3-4a15-967e-4947e8635c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset reference\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "url_request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "# Load the json file and map each headline to whether or not it is sarcastic or not\n",
    "sentences = []\n",
    "labels = []\n",
    "with open('sarcasm.json', 'r') as file:\n",
    "    data = json_load(file)\n",
    "\n",
    "    for item in data:\n",
    "        sentences.append(item['headline'])\n",
    "        labels.append(item['is_sarcastic'])\n",
    "\n",
    "# Set the maximum number of words to keep, based on word frequency. Only the most common `vocab_size` words will be\n",
    "# kept.\n",
    "vocab_size = 1000\n",
    "\n",
    "# Initialize a Tokenizer object with a specified vocabulary size.\n",
    "# `num_words=vocab_size` tells the tokenizer to only use the `vocab_size` most common words.\n",
    "# `oov_token=\"<OOV>\"` sets the token to be used for out-of-vocabulary words. Words not seen in the training data\n",
    "# will be replaced with \"<OOV>\".\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# Updates the internal vocabulary of the tokenizer based on the list of `sentences`.\n",
    "# This method processes each sentence, tokenizes it, and updates the word index based on the words found in the\n",
    "# sentence.\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convert the list of sentences into sequences of integers. Each integer corresponds to a word's index in the\n",
    "# tokenizer's word index.\n",
    "# This transformation is based on the vocabulary established by the tokenizer during its fit on the training texts.\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Set the dimensionality of the embedding layer. This number represents the size of the vector space in which words\n",
    "# will be embedded.\n",
    "# It defines how many factors or dimensions will be used to represent each word in this space.\n",
    "embedding_dim = 10000\n",
    "\n",
    "# Set the maximum length of sequences. If a sequence is shorter than this length, it will be padded with zeros.\n",
    "# If it's longer, it will be truncated to this length. This ensures that all sequences have the same length.\n",
    "max_length = 120\n",
    "\n",
    "# Define the type of padding to use when sequences are shorter than `max_length`.\n",
    "# 'post' means that if a sequence needs padding, zeros will be added to the end of the sequence.\n",
    "padding_type = 'post'\n",
    "\n",
    "# Pad the sequences to ensure they all have the same length. This is necessary because most deep learning models\n",
    "# require inputs to be of the same size. The `pad_sequences` function achieves this by padding shorter sequences\n",
    "# (with zeros, by default) or truncating longer ones to the `max_length` specified.\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding=padding_type)\n",
    "\n",
    "# Convert the list of labels into a NumPy array for efficient computation and compatibility with various machine\n",
    "# learning libraries.\n",
    "# This is particularly useful because NumPy arrays offer a wide range of mathematical operations and are optimized\n",
    "# for performance.\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Define the size of the training dataset. In this case, the first 20,000 examples will be used for training.\n",
    "training_size = 20000\n",
    "\n",
    "# Split the feature data into training and testing sets.\n",
    "# `X_train` will contain the first `training_size` examples, and `X_test` will contain the remaining examples.\n",
    "# This is done by slicing the array `X` using the `training_size` variable.\n",
    "train_data, test_data = X[:training_size], X[training_size:]\n",
    "\n",
    "# Similarly, split the label data (`labels`) into training and testing sets.\n",
    "# train_labels will contain the labels corresponding to the training data, and test_labels will contain the labels\n",
    "# for the testing data.\n",
    "# The labels are split in the same manner as the feature data, ensuring corresponding features and labels are\n",
    "# matched in both training and testing sets.\n",
    "train_labels, test_labels = labels[:training_size], labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a58f2e-353f-4566-bf46-cbfdeaf3bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_data, test_data, train_labels, test_labels):\n",
    "    # Set the dimensionality of the output space for the Embedding layer. This value determines the size of the\n",
    "    # embedding vectors.\n",
    "    output_dim = 32\n",
    "\n",
    "    # Define the neural network model as a sequential model, which means that each layer has exactly one input tensor\n",
    "    # and one output tensor.\n",
    "    model = models.Sequential([\n",
    "        # The Embedding layer transforms integer-encoded vocabulary into dense vector embeddings.\n",
    "        # * `input_dim` is the size of the vocabulary, which should be `vocab_size + 1` because index 0 is reserved for\n",
    "        # padding.\n",
    "        # * `output_dim` is the dimension of the dense embedding.\n",
    "        # * `input_length` is the length of input sequences.\n",
    "        layers.Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, input_length=max_length),\n",
    "\n",
    "        # Dropout layer randomly sets input units to 0 with a frequency of 0.2 at each step during training, which helps\n",
    "        # prevent overfitting.\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Conv1D layer for convolution over the 1D sequence. It uses 32 filters and a kernel size of 5.\n",
    "        # The activation function 'relu' introduces non-linearity to the learning process, allowing the model to learn\n",
    "        # more complex patterns.\n",
    "        layers.Conv1D(32, 5, activation='relu'),\n",
    "\n",
    "        # MaxPooling1D layer reduces the dimensionality of the input by taking the maximum value over a window (of size\n",
    "        # 4 here) for each dimension along the features' axis.\n",
    "        layers.MaxPooling1D(pool_size=4),\n",
    "\n",
    "        # Bidirectional LSTM layer processes the sequence both forwards and backwards (bidirectionally) with 64 units.\n",
    "        # `return_sequences=True` makes the layer return the full sequence of outputs for each input, necessary for\n",
    "        # stacking with other sequence-processing layers.\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "\n",
    "        # GlobalAveragePooling1D layer computes the average of the input's dimensions, reducing its dimensionality and\n",
    "        # preparing it for the dense layer.\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "\n",
    "        # Dense layer with 128 neurons and 'relu' activation function, introducing another level of non-linearity and\n",
    "        # allowing the network to learn more complex representations.\n",
    "        layers.Dense(128, activation='relu'),\n",
    "\n",
    "        # Another Dropout layer, this time with a higher dropout rate of 0.5, to further mitigate the risk of\n",
    "        # overfitting, especially given the larger number of neurons in the preceding Dense layer.\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        # Final Dense output layer with a single neuron, using the 'sigmoid' activation function to output a value\n",
    "        # between 0 and 1,\n",
    "        # which can be interpreted as the probability of the input being in a particular class (e.g., sarcastic or not\n",
    "        # sarcastic in this context).\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model by specifying the optimizer, loss function, and metrics to monitor.\n",
    "    # * 'adam' optimizer is a popular choice for many types of neural networks due to its adaptive learning rate\n",
    "    # capabilities.\n",
    "    # * 'binary_crossentropy' is suitable for binary classification problems. It measures the performance of a\n",
    "    # classification model whose output is a probability value between 0 and 1.\n",
    "    # * 'metrics=['accuracy']': explained inline with other possibilities\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            # 'accuracy': Computes the accuracy rate across all predictions for classification problems.\n",
    "            # It's the number of correct predictions divided by the total number of predictions.\n",
    "            'accuracy',\n",
    "\n",
    "            # 'precision': Measures the proportion of true positive predictions in the positive predictions made by the\n",
    "            # model.\n",
    "            # It is particularly useful when the cost of a false positive is high.\n",
    "            # 'precision' = True Positives / (True Positives + False Positives)\n",
    "\n",
    "            # 'recall': Measures the proportion of true positive predictions in the actual positive labels.\n",
    "            # It is important when the cost of a false negative is high.\n",
    "            # 'recall' = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "            # 'f1-score': Harmonic mean of precision and recall. It's a better measure than accuracy for imbalanced\n",
    "            # classes.\n",
    "            # 'f1-score' = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            # 'auc': Area Under the ROC Curve. AUC provides an aggregate measure of performance across all\n",
    "            # classification thresholds.\n",
    "            # One way of interpreting AUC is as the probability that the model ranks a random positive example more\n",
    "            # highly than a random negative example.\n",
    "\n",
    "            # 'mean_squared_error' or 'mse': Measures the average of the squares of the errors between actual and\n",
    "            # predicted values.\n",
    "            # It is used for regression problems.\n",
    "\n",
    "            # 'mean_absolute_error' or 'mae': Measures the average of the absolute differences between actual and\n",
    "            # predicted values.\n",
    "            # It provides a linear score without direction, meaning the lower the better, regardless of under or\n",
    "            # over forecasting.\n",
    "\n",
    "            # 'mean_absolute_percentage_error' or 'mape': Measures the average of the absolute percentage errors by\n",
    "            # comparing the prediction with the actual value.\n",
    "            # This can be more interpretable in terms of percentage, but can be skewed by small denominators.\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set the number of epochs, which is the number of complete passes through the training dataset.\n",
    "    epochs = 10\n",
    "    # Set the batch size, which is the number of training examples utilized in one iteration.\n",
    "    batch_size = 32\n",
    "\n",
    "    # Fit the model to the training data.\n",
    "    # * 'train_data' and 'train_labels' are the features and labels for the training dataset, respectively.\n",
    "    # * 'epochs=epochs' specifies how many times the learning algorithm will work through the entire training dataset.\n",
    "    # * 'batch_size=batch_size' determines the number of samples per gradient update for training.\n",
    "    # * 'validation_data=(test_data, test_labels)' provides the validation dataset that the model will evaluate its\n",
    "    # performance on at the end of each epoch.\n",
    "    model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_labels))\n",
    "\n",
    "    # Evaluate the model's performance on the test dataset, which it has not seen during training.\n",
    "    # This function returns the loss value and metrics (accuracy in this case) for the model in test mode.\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "    # Print the test accuracy, multiplying by 100 to convert from a proportion to a percentage.\n",
    "    # The formatted string uses {:.2f} to round the accuracy to two decimal places.\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caec035-d518-4c53-abbf-d27f8b20633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path='../models/exercise_4.h5'\n",
    "quantized_model_path='../models/exercise_4.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7580971b-397b-4f43-b4f0-2e26a2118f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 05:47:33.844449: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.871939: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.872022: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.873848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.873911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.873950: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.912709: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.912780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.912833: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:47:33.912878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22100 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 05:47:35.435595: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-03-02 05:47:35.950497: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f1ea35ab040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-02 05:47:35.950508: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-02 05:47:35.953145: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709376456.004140   18260 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 15s 20ms/step - loss: 0.4820 - accuracy: 0.7599 - val_loss: 0.3988 - val_accuracy: 0.8189\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3594 - accuracy: 0.8389 - val_loss: 0.3909 - val_accuracy: 0.8195\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3102 - accuracy: 0.8666 - val_loss: 0.4101 - val_accuracy: 0.8247\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2685 - accuracy: 0.8857 - val_loss: 0.4156 - val_accuracy: 0.8183\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2254 - accuracy: 0.9072 - val_loss: 0.4380 - val_accuracy: 0.8201\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1920 - accuracy: 0.9240 - val_loss: 0.5338 - val_accuracy: 0.8156\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.1573 - accuracy: 0.9366 - val_loss: 0.5681 - val_accuracy: 0.8141\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1327 - accuracy: 0.9485 - val_loss: 0.6277 - val_accuracy: 0.8107\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1167 - accuracy: 0.9551 - val_loss: 0.6753 - val_accuracy: 0.8073\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1018 - accuracy: 0.9604 - val_loss: 0.7238 - val_accuracy: 0.8150\n",
      "210/210 [==============================] - 1s 5ms/step - loss: 0.7238 - accuracy: 0.8150\n",
      "Test Accuracy: 81.50%\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = create_model(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44d54d5-1989-43df-a47d-6ae0f3dbf73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaniganp/mambaforge/envs/tensorflow-exercise-2/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30b66ca-a08d-4f30-9bc6-d8b6c8e262b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 140181776 bytes, or 133.69 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(model_file_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2990c43-239f-47e3-b966-08d7bcfe16b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Sentence: trump again proves his claim about waiting for 'facts' after charlottesville was garbage\n",
      "Is Sarcastic: No\n",
      "\n",
      "Sentence: rnc attendee excited to find out what he'll get to boo tonight\n",
      "Is Sarcastic: Yes\n",
      "\n",
      "Sentence: pixies release secret song for record store day\n",
      "Is Sarcastic: No\n",
      "\n",
      "Sentence: ted cruz jokes about hillary clinton sitting in federal prison\n",
      "Is Sarcastic: No\n",
      "\n",
      "Sentence: advisors tell trump, cruz to stick to just attacking all women in general\n",
      "Is Sarcastic: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sarcasm(model, sentences):\n",
    "    predictions = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize and pad the sentence, same as before\n",
    "        seq = tokenizer.texts_to_sequences([sentence])\n",
    "        padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "        padded = padded.astype(np.float32)  # Ensure the input type matches the model's expectation\n",
    "        \n",
    "        # Use the model's predict method directly\n",
    "        output_data = model.predict(padded)\n",
    "        \n",
    "        # Interpret the model's output\n",
    "        predictions.append((sentence, output_data[0][0] > 0.5))  # Assuming a sigmoid output layer\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Load and preprocess your sentences as before\n",
    "# Example: sentences = ['This is sarcasm.', 'This is not sarcasm.']\n",
    "# Make sure `sentences` variable is defined and populated\n",
    "\n",
    "# Randomly select 5 sentences\n",
    "selected_sentences = random.sample(sentences, 5)\n",
    "\n",
    "# Predict sarcasm for the selected sentences using the Keras model\n",
    "predictions = predict_sarcasm(model, selected_sentences)\n",
    "\n",
    "for sentence, is_sarcastic in predictions:\n",
    "    print(f\"Sentence: {sentence}\\nIs Sarcastic: {'Yes' if is_sarcastic else 'No'}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cca410-a7e6-4661-b2bb-5869ae7b5b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6bx70v31/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6bx70v31/assets\n",
      "2024-03-02 05:49:19.505675: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-02 05:49:19.505688: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-02 05:49:19.505873: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp6bx70v31\n",
      "2024-03-02 05:49:19.516539: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-02 05:49:19.516549: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp6bx70v31\n",
      "2024-03-02 05:49:19.540645: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-03-02 05:49:19.548021: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-02 05:49:19.650307: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp6bx70v31\n",
      "2024-03-02 05:49:19.699722: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 193848 microseconds.\n",
      "2024-03-02 05:49:19.976469: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-03-02 05:49:19.976513: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "2024-03-02 05:49:19.976523: E tensorflow/compiler/mlir/lite/stablehlo/transforms/op_stat_pass.cc:119] Unsupported data type.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 37, Total Ops 118, % non-converted = 31.36 %\n",
      " * 32 ARITH ops, 5 TF ops\n",
      "\n",
      "- arith.constant:   32 occurrences  (f32: 16, i32: 16)\n",
      "\n",
      "  (i1: 2, i32: 2)\n",
      "\n",
      "\n",
      "- tf.TensorListReserve:    1 occurrences  (: 1)\n",
      "- tf.TensorListSetItem:    2 occurrences  (: 2)\n",
      "- tf.TensorListStack:    2 occurrences  (f32: 2)\n",
      "  (f32: 6, i32: 4)\n",
      "  (i32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 6)\n",
      "  (f32: 3)\n",
      "  (i1: 2)\n",
      "  (f32: 7)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 6)\n",
      "\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "  (f32: 4)\n",
      "  (f32: 3)\n",
      "  (i32: 2)\n",
      "\n",
      "2024-03-02 05:49:20.024747: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2921] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<?x?x64xf32>) : {device = \"\", num_elements = -1 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS, lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Generate a quantized model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open(quantized_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddfcb826-88dd-4965-b526-da18937163ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model size: 46725832 bytes, or 44.56 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Quantized Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "083ac090-c781-40c3-851d-a55de7eb04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 05:54:50.729497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:54:50.729595: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:54:50.729634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:54:50.729695: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:54:50.729734: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-02 05:54:50.729768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22100 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.53%\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Variable to store the results\n",
    "accurate_count = 0\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    # Preprocess the test data to fit the model input\n",
    "    test_sample = test_data[i].reshape(input_details[0]['shape'])\n",
    "\n",
    "    # IMPORTANT: Cast the test sample to FLOAT32, as expected by the model\n",
    "    test_sample = test_sample.astype(np.float32)\n",
    "\n",
    "    # Set the tensor to point to the input data to be inferred\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_sample)\n",
    "\n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Retrieve the model's prediction\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # Apply a threshold to the output to get the predicted class (if your model is for classification)\n",
    "    predicted_class = (output_data > 0.5).astype(int)[0][0]\n",
    "\n",
    "    # Compare with the actual class\n",
    "    if predicted_class == test_labels[i]:\n",
    "        accurate_count += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accurate_count / len(test_data)\n",
    "\n",
    "# Print the test accuracy, multiplying by 100 to convert from a proportion to a percentage.\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dacf2116-d8a0-4627-ba27-55394c972ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm(interpreter, sentences):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    predictions = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize and pad the sentence\n",
    "        seq = tokenizer.texts_to_sequences([sentence])\n",
    "        padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "        padded = padded.astype(np.float32)  # Ensure the input type matches the model's expectation\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], padded)\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append((sentence, output_data[0][0] > 0.5))  # Assuming a sigmoid output layer\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe35d36e-b5b1-420d-bf59-0b7d9cd8d804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: report: 1 in 5 air ducts contains person looking, listening in on you\n",
      "Is Sarcastic: Yes\n",
      "\n",
      "Sentence: rosie perez wants to understand why anyone would vote for trump\n",
      "Is Sarcastic: No\n",
      "\n",
      "Sentence: cross-shaped wwi monument declared unconstitutional\n",
      "Is Sarcastic: Yes\n",
      "\n",
      "Sentence: barksdale, inspiration behind characters on 'the wire,' dies in federal prison\n",
      "Is Sarcastic: No\n",
      "\n",
      "Sentence: republicans admit tax reform won't benefit all middle-class households\n",
      "Is Sarcastic: No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 sentences\n",
    "selected_sentences = random.sample(sentences, 5)\n",
    "\n",
    "# Predict sarcasm for the selected sentences\n",
    "predictions = predict_sarcasm(interpreter, selected_sentences)\n",
    "\n",
    "for sentence, is_sarcastic in predictions:\n",
    "    print(f\"Sentence: {sentence}\\nIs Sarcastic: {'Yes' if is_sarcastic else 'No'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f2bc9-ca4e-4949-bb6c-4d71289a1bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
