{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fcb8d-2613-415d-84d4-8b42e710907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice to have and only here as a reference until moved to its instructional home :)\n",
    "#export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn; print(nvidia.cudnn.__file__)\"))\n",
    "#export SITE_PACKAGES_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\n",
    "#export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$SITE_PACKAGES_PATH/tensorrt_libs/:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6cf28-70a4-4875-b109-78836094c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fbfae-40f2-4b22-8698-deeed76317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment: This is a more extensive write-up of the much faster exercise_2_old in archive. It is mean to begin learning\n",
    "# the ins and outs of training and evaluating a tensorflow model workflow.\n",
    "\n",
    "# The os module in Python provides a way of using operating system dependent functionality. It allows you to interface\n",
    "# with the underlying operating system that Python is running on – be it Windows, Mac or Linux. You can use the os module\n",
    "# to handle file and directory paths, create folders, list contents of a directory, manage environment variables, execute\n",
    "# shell commands, and more.\n",
    "import os\n",
    "\n",
    "# The 'matplotlib.pyplot' is a collection of functions in the 'matplotlib' library that make matplotlib work like MATLAB.\n",
    "# Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure,\n",
    "# plots some lines in a plotting area, decorates the plot with labels, etc. 'plt' is a commonly used shorthand alias\n",
    "# for 'matplotlib.pyplot'. This allows you to access matplotlib's plotting functions with shorter syntax - for example,\n",
    "# you can type 'plt.plot()' instead of 'matplotlib.pyplot.plot()'. This import is essential for data visualization,\n",
    "# allowing you to create a wide variety of static, animated, and interactive plots and charts in Python.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy (Numerical Python) is an essential library for scientific computing in Python. It provides support for large,\n",
    "# multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "# With NumPy, you can perform complex mathematical operations on large amounts of data with ease and efficiency. It's\n",
    "# widely used in data analysis, machine learning, and engineering for its speed and versatility. Importing it as 'np'\n",
    "# is a common convention, making the code more readable and easier to write for the Python community.\n",
    "import numpy as np\n",
    "\n",
    "# A standard Python module for generating random numbers.\n",
    "from random import randint\n",
    "\n",
    "# TensorFlow is an open-source machine learning library developed by Google. It's used for both research and production\n",
    "# at Google.\n",
    "# * data: primarily used for data preprocessing and pipeline building. It offers tools for reading and writing data in\n",
    "#   various formats, transforming it, and making it ready for machine learning models. Efficient data handling is\n",
    "#   crucial in machine learning workflows, and TensorFlow's data module simplifies this process significantly.\n",
    "from tensorflow import data\n",
    "# * keras: originally an independent neural network library, now integrated into TensorFlow,\n",
    "#   simplifies the creation and training of deep learning models. Keras is known for its user-friendliness and modular\n",
    "#   approach, allowing for easy and fast prototyping. It provides high-level building blocks for developing deep\n",
    "#   learning models while still enabling users to dive into lower-level operations if needed.\n",
    "from tensorflow import keras\n",
    "# * tensorflow.python.client: Provides functionalities to query the properties of the hardware devices TensorFlow can\n",
    "#   access. Specifically, this module is often used to list and get detailed information about the system's available\n",
    "#   CPUs, GPUs, and other hardware accelerators compatible with TensorFlow.\n",
    "from tensorflow.python.client import device_lib\n",
    "# * keras.models: This module in Keras is essential for creating neural network models. It includes classes like\n",
    "#   Sequential and the Functional API for building models. The Sequential model is straightforward, allowing layers to\n",
    "#   be added in sequence, suitable for simple architectures. The Functional API, on the other hand, provides greater\n",
    "#   flexibility for creating complex models with advanced features like shared layers and multiple inputs/outputs.\n",
    "#   Both types enable comprehensive model management, including training, evaluation, and saving/loading\n",
    "#   functionalities, making them versatile for a wide range of deep learning applications.\n",
    "from keras.models import load_model, Sequential\n",
    "# * keras.callbacks: The keras.callbacks module offers a set of tools that can be applied during the training process of\n",
    "#   a model. These callbacks are used for various purposes like monitoring the model's performance in real-time, saving\n",
    "#   the model at certain intervals, early stopping when the performance plateaus, adjusting learning rates, and more.\n",
    "#   They are crucial for enhancing and controlling the training process, allowing for automated and optimized model\n",
    "#   training. Callbacks like ModelCheckpoint, EarlyStopping, TensorBoard, and ReduceLROnPlateau are commonly used for\n",
    "#   efficient model training and fine-tuning.\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Versioning sourcing\n",
    "from tensorflow import __version__ as tf_version\n",
    "\n",
    "# TensorFlow Lite provides tools and classes for converting TensorFlow models into a highly optimized format suitable\n",
    "# for deployment on mobile devices, embedded systems, or other platforms with limited computational capacity. This\n",
    "# module includes functionalities for model conversion, optimization, and inference. By importing `lite`, you gain\n",
    "# access to the TFLiteConverter class for model conversion, optimization options like quantization, and utilities for\n",
    "# running TFLite models on target devices.\n",
    "from tensorflow import lite\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "# This is a convenient module provided by Keras that contains various datasets used for machine learning tasks.\n",
    "# These datasets are preprocessed and ready to use, making them ideal for educational purposes, benchmarking, and quick\n",
    "# prototyping. In the context of your code, datasets is used to access the Fashion MNIST dataset, which is a collection\n",
    "# of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset is a\n",
    "# more complex, yet similar, alternative to the classic MNIST dataset of handwritten digits.\n",
    "from keras import datasets, layers\n",
    "\n",
    "# Mixed Precision: By combining float16 and float32 data types in your model, you can speed up training and reduce\n",
    "# memory usage while maintaining the model's accuracy. This is especially beneficial when training large models or\n",
    "# working with large datasets.\n",
    "from keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Regular Expressions\n",
    "# 1. search: This function is used to perform a search for a pattern in a string and returns a match object if the\n",
    "# pattern is found, otherwise None. It's particularly useful for string pattern matching and extracting specific\n",
    "# segments from text.\n",
    "from re import search\n",
    "\n",
    "# Key aspects of 'check_output':\n",
    "# 1. **Process Execution**: The 'check_output' function is used to run a command in the subprocess/external process and\n",
    "#    capture its output. This is especially useful for running system commands and capturing their output directly\n",
    "#    within a Python script.\n",
    "# 2. **Return Output**: It returns the output of the command, making it available to the Python environment. If the\n",
    "#    called command results in an error (non-zero exit status), it raises a CalledProcessError.\n",
    "# 3. **Use Cases**: Common use cases include executing a shell command, reading the output of a command, automating\n",
    "#    scripts that interact with the command line, and integrating external tools into a Python workflow.\n",
    "# Example Usage:\n",
    "# Suppose you want to capture the output of the 'ls' command in a Unix/Linux system. You can use 'check_output' like\n",
    "# this:\n",
    "# output = check_output(['ls', '-l'])\n",
    "from subprocess import check_output\n",
    "# Key aspects of 'CalledProcessError':\n",
    "#  1. Error Handling: CalledProcessError is an exception raised by check_output when the command it tries to execute\n",
    "#   returns a non-zero exit status, indicating failure. This exception is particularly useful for error handling in\n",
    "#   scripts where the success of an external command is crucial.\n",
    "#  2. Exception Details: The exception object contains information about the error, including the return code, command\n",
    "#  executed, and output (if any). This aids in debugging by providing clear insights into why the external command\n",
    "#  failed.\n",
    "#  3. Handling the Exception: In practical use, it is often caught in a try-except block, allowing the script to respond\n",
    "#  appropriately to the failure of the external command, like logging the error or trying a fallback operation.\n",
    "from subprocess import CalledProcessError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b393f-78aa-4f8b-8547-27593ce99fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function `print_gpu_info` is designed to display detailed information about the available GPUs on the system.\n",
    "# It utilizes TensorFlow's `device_lib.list_local_devices()` method to enumerate all computing devices recognized by\n",
    "# TensorFlow. For each device identified as a GPU, the function extracts and prints relevant details including the GPU's\n",
    "# ID, name, memory limit (converted to megabytes), and compute capability. The extraction of GPU information involves\n",
    "# parsing the device's description string using regular expressions to find specific pieces of information. This\n",
    "# function can be particularly useful for debugging or for setting up configurations in environments with multiple GPUs,\n",
    "# ensuring that TensorFlow is utilizing the GPUs as expected.\n",
    "\n",
    "def print_gpu_info():\n",
    "    # Undocumented Method\n",
    "    # https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    # Get the list of all devices\n",
    "    devices = device_lib.list_local_devices()\n",
    "\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            # Extract the physical device description\n",
    "            desc = device.physical_device_desc\n",
    "\n",
    "            # Use regular expressions to extract the required information\n",
    "            gpu_id_match = search(r'device: (\\d+)', desc)\n",
    "            name_match = search(r'name: (.*?),', desc)\n",
    "            compute_capability_match = search(r'compute capability: (\\d+\\.\\d+)', desc)\n",
    "\n",
    "            if gpu_id_match and name_match and compute_capability_match:\n",
    "                gpu_id = gpu_id_match.group(1)\n",
    "                gpu_name = name_match.group(1)\n",
    "                compute_capability = compute_capability_match.group(1)\n",
    "\n",
    "                # Convert memory limit from bytes to gigabytes and round it\n",
    "                memory_limit_gb = round(device.memory_limit / (1024 ** 2))\n",
    "\n",
    "                print(\n",
    "                    f\"\\tGPU ID {gpu_id} --> {gpu_name} --> \"\n",
    "                    f\"Memory Limit {memory_limit_gb} MB --> \"\n",
    "                    f\"Compute Capability {compute_capability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7796a-231e-4f75-8bb8-0a3e8b6f9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA Driver\n",
    "try:\n",
    "    # Execute the nvidia-smi command and decode the output\n",
    "    nvidia_smi_output = check_output(\"nvidia-smi\", shell=True).decode()\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = nvidia_smi_output.split('\\n')\n",
    "\n",
    "    # Find the line containing the driver version\n",
    "    driver_line = next((line for line in lines if \"Driver Version\" in line), None)\n",
    "\n",
    "    # Extract the driver version number\n",
    "    if driver_line:\n",
    "        driver_version = driver_line.split('Driver Version: ')[1].split()[0]\n",
    "        print(\"NVIDIA Driver:\", driver_version)\n",
    "\n",
    "        # Extract the maximum supported CUDA version\n",
    "        cuda_version = driver_line.split('CUDA Version: ')[1].strip().replace(\"|\", \"\")\n",
    "        print(\"Maximum Supported CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"NVIDIA Driver Version or CUDA Version not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching NVIDIA Driver Version or CUDA Version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce4cc0-364c-4b85-9dcd-64505b79a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Software Versions:\")\n",
    "\n",
    "# CUDA\n",
    "try:\n",
    "    # Execute the 'nvcc --version' command and decode the output\n",
    "    nvcc_output = check_output(\"nvcc --version\", shell=True).decode()\n",
    "\n",
    "    # Use regular expression to find the version number\n",
    "    match = search(r\"V(\\d+\\.\\d+\\.\\d+)\", nvcc_output)\n",
    "    if match:\n",
    "        cuda_version = match.group(1)\n",
    "        print(\"CUDA Version\", cuda_version)\n",
    "    else:\n",
    "        print(\"CUDA Version not found\")\n",
    "\n",
    "except CalledProcessError as e:\n",
    "    print(\"Error executing nvcc --version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061669c3-5c5f-467f-9b69-69b4bd046781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the global dtype policy to the specified policy. In your code, set_global_policy('mixed_float16') is used to set\n",
    "# the global policy to 'mixed_float16', which means that layers with compute dtype float16 and variable dtype float32\n",
    "# will be created by default. This can lead to significant performance gains, particularly on modern GPUs.\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8cb1-6a5a-4809-baa5-ec18859c3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names for Fashion MNIST\n",
    "class_names = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "# Get the key length of the dictionary of Fashion MNIST\n",
    "num_classes = len(class_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918e773-2e9e-4946-beb4-a1283d466565",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path='../models/exercise_2.h5'\n",
    "quantized_model_path='../models/exercise_2.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa362c0-b182-45e3-ba3f-c53a4dac833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST dataset, which is a collection of 60,000 28x28 grayscale images of 10 fashion categories, along with\n",
    "# a test set of 10,000 images.\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3c783-3e70-478c-97fe-3a9ccf647ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_file_path, train_images, train_labels, test_images, test_labels, epochs):\n",
    "    # What is One-Hot Encoding?\n",
    "    # One-hot encoding is a method to convert categorical variables into a numerical form. This process is essential for\n",
    "    # preparing categorical data for many kinds of machine learning algorithms, which require numerical input.\n",
    "\n",
    "    # How Does One-Hot Encoding Work?\n",
    "    # In one-hot encoding, each categorical value is converted into a binary vector. This vector has one 'hot' or\n",
    "    # 'active' state (represented by 1) and the rest 'cold' states (represented by 0s). The length of this vector is\n",
    "    # equal to the number of categories in the dataset.\n",
    "\n",
    "    # Why Use One-Hot Encoding?\n",
    "    # Many machine learning algorithms cannot work directly with categorical data. One-hot encoding converts the\n",
    "    # categories into a form that these algorithms can understand and process.\n",
    "\n",
    "    # Application in Your Code:\n",
    "    # Here, one-hot encoding transforms these labels into a binary matrix representation. For instance, a label '2' will\n",
    "    # be converted to [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] in a 10-dimensional space (since there are 10 categories).\n",
    "\n",
    "    # Convert the labels to one-hot encoding\n",
    "    train_labels_one_hot = keras.utils.to_categorical(train_labels, num_classes)\n",
    "    test_labels_one_hot = keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "    # What is AUTOTUNE?\n",
    "    # AUTOTUNE is a special value in TensorFlow's tf.data API. It's used to dynamically adjust the performance-related\n",
    "    # configuration settings in the data loading pipeline. Essentially, it allows TensorFlow to automatically determine\n",
    "    # the optimal number of elements to prefetch or other optimization parameters based on the available CPU and GPU\n",
    "    # resources.\n",
    "    # Why Use AUTOTUNE?\n",
    "\n",
    "    # The main goal of AUTOTUNE is to optimize the data pipeline's performance. In machine learning, especially with\n",
    "    # large datasets or complex models, data loading can become a bottleneck. AUTOTUNE helps to alleviate this by\n",
    "    # optimizing the data processing steps, leading to faster and more efficient training.\n",
    "    # How Does AUTOTUNE Work?\n",
    "\n",
    "    # When AUTOTUNE is set, TensorFlow monitors the data pipeline during runtime and adjusts the buffer sizes and other\n",
    "    # settings for operations like prefetching, caching, and shuffling. This dynamic adjustment is based on the current\n",
    "    # system's resource availability and workload, aiming to optimize throughput and reduce latency.\n",
    "\n",
    "    # Advantages:\n",
    "    # Improved Performance: By allowing TensorFlow to optimize prefetch buffer sizes, the I/O time decreases, and the\n",
    "    # data feeding pipeline becomes more efficient.\n",
    "    # Resource Management: It helps in better utilization of CPU and GPU resources by dynamically adjusting to the\n",
    "    # system's available resources.\n",
    "    # Ease of Use: Instead of manually tuning the performance parameters, AUTOTUNE simplifies the process, making it\n",
    "    # easier to work with complex data pipelines.\n",
    "\n",
    "    # Apply AUTOTUNE to dataset operations\n",
    "    batch_size = 32\n",
    "    train_dataset = data.Dataset.from_tensor_slices((train_images, train_labels_one_hot))\n",
    "    train_dataset = (train_dataset.\n",
    "                     cache().  # This method caches the dataset in memory. Once the dataset is loaded into memory,\n",
    "                     # subsequent iterations over the data will be much faster.\n",
    "                     shuffle(len(train_images)).  # This method randomly shuffles the elements of the dataset.\n",
    "                     # The len(train_images) argument specifies the buffer size for shuffling.\n",
    "                     batch(batch_size).  # This method combines consecutive elements of the dataset into batches.\n",
    "                     # Here, it's creating batches of 32 elements (images and labels).\n",
    "                     prefetch(data.AUTOTUNE))  # This is where AUTOTUNE is applied. Prefetching allows later elements of\n",
    "    # the dataset to be prepared while the current element is being processed. This can improve the efficiency of the\n",
    "    # pipeline. By setting it to AUTOTUNE, you're allowing TensorFlow to automatically manage the buffer size for\n",
    "    # prefetching based on runtime conditions.\n",
    "\n",
    "    # Apply AUTOTUNE to test dataset operations\n",
    "    test_dataset = data.Dataset.from_tensor_slices((test_images, test_labels_one_hot))\n",
    "    test_dataset = test_dataset.batch(batch_size).cache().prefetch(data.AUTOTUNE)\n",
    "\n",
    "    # Example: printing one-hot encoded labels for the first training sample\n",
    "    print(\"Original label: \", train_labels[0])\n",
    "    print(\"One-Hot Encoded Label: \", train_labels_one_hot[0])\n",
    "\n",
    "    # Get a random index\n",
    "    random_index = randint(0, len(train_images) - 1)\n",
    "\n",
    "    # Get the random image and its label\n",
    "    random_image = train_images[random_index]\n",
    "    random_label = train_labels[random_index]\n",
    "\n",
    "    # Print the label and its corresponding class name\n",
    "    print(\"Label:\", random_label)\n",
    "    print(\"Class Name:\", class_names[random_label])\n",
    "\n",
    "    # Print the shape of the image\n",
    "    print(\"Image Shape:\", random_image.shape)\n",
    "\n",
    "    # Normalize images (get values between 0 & 1)\n",
    "    train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "    # Check the shape of the input data\n",
    "    print(f\"Print the train image shape {train_images.shape}\")\n",
    "    print(f\"print the train labels shape {train_labels.shape}\")\n",
    "\n",
    "    # Assuming the input shape is (height, width), for example, (28, 28) MNIST dataset\n",
    "    # Tuple Unpacking: In Python, the asterisk (*) is used to unpack a tuple. This means that the elements of the tuple\n",
    "    # are \"taken out\" and used individually.\n",
    "    input_shape = (28, 28)\n",
    "    # Tuple Extension: By placing the unpacked tuple and another element (in this case, 1) inside parentheses and\n",
    "    # separating them by a comma, a new tuple is created that extends the original tuple.\n",
    "    reshaped_input_shape = (*input_shape, 1)\n",
    "\n",
    "    # Build the model\n",
    "    # This is a Keras model that linearly stacks layers. It's ideal for a plain stack of layers where each layer has\n",
    "    # exactly one input tensor and one output tensor.\n",
    "\n",
    "    model_2 = Sequential([\n",
    "        # Reshape Layer\n",
    "        # Adjusts the input data shape to a format suitable for convolutional layers. It's crucial when your input data\n",
    "        # does not already have the required dimensions (like adding a channel dimension to grayscale images).\n",
    "        layers.Reshape(reshaped_input_shape, input_shape=input_shape),\n",
    "\n",
    "        # First convolutional layer\n",
    "        layers.Conv2D(\n",
    "            32,  # Number of filters (or kernels). Each filter extracts different features from the input image.\n",
    "            3,  # Kernel size (3x3). This size is a common choice for extracting features.\n",
    "            padding='same',  # Padding means the output size is the same as the input size. This is achieved by adding\n",
    "            # padding to the input.\n",
    "            activation=\"relu\"  # (Rectified Linear Unit) is a common activation function that introduces non-linearity\n",
    "            # to the model, allowing it to learn more complex patterns.\n",
    "        ),\n",
    "\n",
    "        # Pool Size: (2, 2) reduces the spatial dimensions (height and width) of the output from the previous layer.\n",
    "        # It helps in reducing the computational load and overfitting.\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        # See first comment for first convolutional layer\n",
    "        layers.Conv2D(32, 3, padding='same', activation=\"relu\"),\n",
    "\n",
    "        # Pool Size: (2, 2) reduces the spatial dimensions (height and width) of the output from the previous layer.\n",
    "        # It helps in reducing the computational load and overfitting.\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        # See first comment for first convolutional layer\n",
    "        layers.Conv2D(16, 3, padding='same', activation=\"relu\"),\n",
    "\n",
    "        # Pool Size: (2, 2) reduces the spatial dimensions (height and width) of the output from the previous layer.\n",
    "        # It helps in reducing the computational load and overfitting.\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        # Converts the 2D output of the previous layers into a 1D array. This is necessary because the next dense layer\n",
    "        # expects a 1D input.\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dense(\n",
    "            num_classes,  # Number of neurons. This should be equal to the number of classes in the classification task.\n",
    "            activation=\"softmax\",  # \"softmax\" is used for multi-class classification. It outputs a probability\n",
    "            # distribution over the classes.\n",
    "            dtype='float32'  # 'float32'. This is explicitly set due to mixed precision training (combining float16 and\n",
    "            # float32 for performance).\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # The compile method of a Keras model is where you configure the learning process before training the model. This\n",
    "    # method is essential as it sets up the optimizer, loss function, and metrics to be used for training and evaluating\n",
    "    # the model. Let's dissect each component of the compile method in your code:\n",
    "    model_2.compile(\n",
    "        # categorical_crossentropy: This is the loss function used for multi-class classification problems where labels\n",
    "        # are provided in a one-hot encoded format. The categorical cross-entropy loss compares the distribution of the\n",
    "        # predictions (the outputs of the softmax in your model) with the true distribution (the true labels of the data\n",
    "        # ). In simpler terms, this function calculates the difference between the actual labels and the predicted\n",
    "        # labels. The goal of training is to minimize this difference.\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        # Adam Optimizer: Adam is an optimization algorithm that can be used instead of the classical stochastic\n",
    "        # gradient descent procedure to update network weights iteratively based on training data.\n",
    "\n",
    "        # Learning Rate (0.001): This is one of the most important hyperparameters in training neural networks. The\n",
    "        # learning rate defines the step size at each iteration while moving toward a minimum of the loss function. A\n",
    "        # learning rate of 0.001 is a good starting point for many models.\n",
    "\n",
    "        # Adam optimizer combines the advantages of two other extensions of stochastic gradient descent, namely Adaptive\n",
    "        # Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It's known for its efficiency and\n",
    "        # effectiveness in practice, especially in situations with large datasets or parameters.\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "\n",
    "        # 1. Accuracy\n",
    "        # * Usage: Widely used in classification problems.\n",
    "        #\n",
    "        # * Explanation: Accuracy is the most intuitive performance measure and it is simply a ratio of correctly\n",
    "        # predicted observations to the total observations. It's a great measure when the target classes are well\n",
    "        # balanced.\n",
    "        #\n",
    "        # * Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN), where TP = True Positives, TN = True Negatives,\n",
    "        # FP = False Positives, and FN = False Negatives.\n",
    "        #\n",
    "        # * Considerations: While accuracy is very intuitive, it can be misleading in cases where the class distribution\n",
    "        # is imbalanced. For example, in a dataset with 95% of Class A and 5% of Class B, a model that always predicts\n",
    "        # Class A will still have a high accuracy of 95%, despite not being useful.\n",
    "\n",
    "        # 2. Mean Absolute Error (MAE)\n",
    "        # * Usage: Commonly used in regression problems but can be adapted for classification.\n",
    "        #\n",
    "        # * Explanation: MAE measures the average magnitude of the errors in a set of predictions, without considering\n",
    "        # their direction. It’s the average over the test sample of the absolute differences between prediction and\n",
    "        # actual observation where all individual differences have equal weight.\n",
    "        #\n",
    "        # * Formula: MAE = (1/n) * Σ|actual - forecast|, where n is the number of observations.\n",
    "        #\n",
    "        # * Considerations: MAE is a linear score, which means all the individual differences are weighted equally in\n",
    "        # the average. For example, in a temperature forecasting model, an MAE of 2 degrees means the average prediction\n",
    "        # was within 2 degrees of the actual temperature. In classification, it can be used to indicate how far off the\n",
    "        # predictions are on average, but it's less common and less intuitive than in regression.\n",
    "        metrics=[\"accuracy\", \"mae\"]\n",
    "\n",
    "        # Other Metrics to consider:\n",
    "\n",
    "        # 1. Precision\n",
    "        # * Usage: Particularly useful in scenarios where False Positives are more concerning than False Negatives.\n",
    "        #\n",
    "        # * Explanation: Precision is the ratio of correctly predicted positive observations to the total predicted\n",
    "        # positives. High precision relates to a low rate of false positives.\n",
    "        #\n",
    "        # * Formula: Precision = TP / (TP + FP), where TP is True Positives and FP is False Positives.\n",
    "        #\n",
    "        # * Consideration: Precision becomes especially important in scenarios where False Positives are costly.\n",
    "        # For example, in email spam detection, a false positive (marking a legitimate email as spam) can be more\n",
    "        # problematic than a false negative (failing to mark a spam email). However, focusing only on precision might\n",
    "        # lead to a model that is overly cautious, potentially missing out on many true positive cases.\n",
    "\n",
    "        # 2. Recall (Sensitivity)\n",
    "        # * Usage: Important in cases where False Negatives are more critical than False Positives.\n",
    "        #\n",
    "        # * Explanation: Recall is the ratio of correctly predicted positive observations to all the observations in the\n",
    "        # actual class. High recall relates to a low rate of false negatives.\n",
    "        #\n",
    "        # * Formula: Recall = TP / (TP + FN), where FN is False Negatives.\n",
    "        #\n",
    "        # * Consideration: Recall should be prioritized in situations where missing a True Positive is more critical\n",
    "        # than incorrectly labeling a False Positive. For example, in medical testing for a serious disease, it's\n",
    "        # crucial to identify all potential cases (high recall), even at the risk of some false alarms\n",
    "        # (lower precision). However, a high recall with very low precision can lead to many false alarms, which might\n",
    "        # be costly or inefficient.\n",
    "\n",
    "        # 3. F1 Score\n",
    "        # * Usage: Useful when you want to balance precision and recall.\n",
    "        #\n",
    "        # * Explanation: The F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both\n",
    "        # false positives and false negatives into account.\n",
    "        #\n",
    "        # * Formula: F1 Score = 2*(Recall * Precision) / (Recall + Precision).\n",
    "        #\n",
    "        # * Consideration: F1 Score is useful when seeking a balance between Precision and Recall, especially in cases\n",
    "        # where an uneven class distribution makes accuracy a less reliable metric. However, the F1 Score treats\n",
    "        # precision and recall as equally important, which might not be suitable for all contexts. For instance, in\n",
    "        # certain scenarios, recall might be significantly more important than precision and vice versa.\n",
    "\n",
    "        # 4. Area Under the Curve (AUC) [BINARY CLASSICATION]\n",
    "        # * Usage: Commonly used in binary classification problems.\n",
    "        #\n",
    "        # * Explanation: AUC represents the ability of the model to distinguish between classes. An AUC of 0.5 suggests\n",
    "        # no discrimination ability, whereas an AUC of 1.0 represents perfect discrimination.\n",
    "        #\n",
    "        # * Consideration: AUC is a powerful metric for binary classification problems, as it provides an aggregate\n",
    "        # measure of performance across all possible classification thresholds. However, AUC doesn't convey information\n",
    "        # about the actual values of thresholds and how they affect the balance between false positives and false\n",
    "        # negatives. It's also less informative in multi-class classification scenarios.\n",
    "\n",
    "        # 5. Mean Squared Error (MSE)\n",
    "        # * Usage: Common in regression problems.\n",
    "        #\n",
    "        # * Explanation: MSE measures the average of the squares of the errors—that is, the average squared difference\n",
    "        # between the estimated values and the actual value.\n",
    "        #\n",
    "        # * Formula: MSE = (1/n) * Σ(actual - forecast)², where n is the number of observations.\n",
    "        #\n",
    "        # * Consideration: MSE is sensitive to outliers as it squares the errors before averaging them. This can lead to\n",
    "        # situations where the model is heavily penalized for a small number of large errors, which might not be\n",
    "        # desirable in all contexts. MSE is more suitable when large errors are particularly undesirable and need more\n",
    "        # attention.\n",
    "\n",
    "        # 6. Root Mean Squared Error (RMSE)\n",
    "        # * Usage: Also common in regression problems.\n",
    "        #\n",
    "        # * Explanation: RMSE is the square root of the mean of the squared errors. It's a measure of how spread out\n",
    "        # these residuals are, i.e., it tells you how concentrated the data is around the line of best fit.\n",
    "        #\n",
    "        # * Formula: RMSE = √[Σ(predicted - actual)² / n].\n",
    "        #\n",
    "        # * Consideration: Similar to MSE, RMSE also gives higher weight to larger errors (due to squaring). This can be\n",
    "        # beneficial when large errors are more significant, but it can also skew the model's focus towards larger\n",
    "        # errors. RMSE can be more interpretable than MSE as it is in the same units as the response variable.\n",
    "\n",
    "        # 7. Logarithmic Loss (Log Loss)\n",
    "        #\n",
    "        # * Usage: Common in binary and multi-class classification problems.\n",
    "        #\n",
    "        # * Explanation: Log loss measures the performance of a classification model where the prediction is a\n",
    "        # probability value between 0 and 1. Log loss increases as the predicted probability diverge from the actual\n",
    "        # label.\n",
    "        #\n",
    "        # * Consideration: Log Loss is sensitive to the predicted probabilities and is a strict measure, heavily\n",
    "        # penalizing confident but wrong predictions. While it's a good measure for evaluating probabilistic outputs,\n",
    "        # its strictness can sometimes lead to misleading conclusions, especially if the model is calibrated to be\n",
    "        # overly confident.\n",
    "    )\n",
    "\n",
    "    # The EarlyStopping callback in Keras is used to stop training the model when a monitored metric has stopped\n",
    "    # improving. This is a form of regularization used to prevent overfitting and to ensure that the model doesn't waste\n",
    "    # computational resources by continuing to train past the point of diminishing returns.\n",
    "\n",
    "    # Benefits\n",
    "    # * Prevents Overfitting: By stopping training once the model performance ceases to improve, EarlyStopping helps to\n",
    "    # prevent the model from overfitting to the training data.\n",
    "    # * Saves Time and Resources: It reduces unnecessary computation and resource usage by cutting short the training\n",
    "    # process when further training is unlikely to improve the model's performance.\n",
    "    # * Optimizes Model Performance: With the restore_best_weights option, the model is guaranteed to exit training with\n",
    "    # the most effective weights it found, rather than potentially overfit weights from the final epochs of training.\n",
    "\n",
    "    # Usage Scenario\n",
    "    # The EarlyStopping callback is particularly useful in scenarios where you don't know beforehand how many epochs are\n",
    "    # needed to train the model. Instead of arbitrarily choosing a large number of epochs and risking overfitting,\n",
    "    # EarlyStopping allows the model to determine the optimal number of epochs based on the actual training dynamics.\n",
    "    #\n",
    "    # It's a smart way to ensure efficient and effective training, especially when working with large and complex\n",
    "    # datasets.\n",
    "\n",
    "    # Best Practices:\n",
    "    # Determining the right value for the patience parameter in EarlyStopping can be somewhat nuanced, as it depends on\n",
    "    # the specific characteristics of your training data and model. However, there are general guidelines and best\n",
    "    # practices you can consider:\n",
    "\n",
    "    # 1. Change in Performance Metric: A standard approach is to monitor a specific performance metric, like validation\n",
    "    # accuracy or loss, and stop training if there's no significant change in this metric over a given number of epochs.\n",
    "    # What constitutes a \"significant change\" can vary; sometimes a minimal improvement threshold (like 1% in accuracy)\n",
    "    # is set to avoid stopping due to minor fluctuations.\n",
    "\n",
    "    # 2. Generalization vs. Training Time: Research suggests that \"slower\" stopping criteria, which allow more epochs\n",
    "    # before stopping, can lead to improved generalization (better performance on unseen data) compared to \"faster\"\n",
    "    # criteria. However, this comes at the cost of longer training times and increased computational resources.\n",
    "\n",
    "    # 3. Monitoring Loss vs. Other Metrics: While it's common to monitor loss, it may not always capture what's most\n",
    "    # important about your model. You might want to choose a performance metric that best defines the model's utility\n",
    "    # for its intended application. For example, in a classification task, you might prioritize accuracy or F1 score\n",
    "    # over loss.\n",
    "\n",
    "    # 4. Learning Curves Analysis: Before implementing early stopping, it's helpful to train your model for a\n",
    "    # sufficiently long time without it and plot learning curves. This can give you insights into how noisy the training\n",
    "    #  process is and help you better decide the patience value.\n",
    "\n",
    "    # 5. Model Complexity and Dataset Size: The optimal patience value can also depend on the complexity of your model\n",
    "    # and the size of your dataset. For simpler models or smaller datasets, a smaller patience might suffice, while more\n",
    "    # complex models or larger datasets might benefit from a larger patience value to fully realize their learning\n",
    "    # potential.\n",
    "\n",
    "    # 6. Early Stopping in Conjunction with Other Techniques: Early stopping is often used in conjunction with other\n",
    "    # regularization techniques like dropout or weight decay. The combined effect of these techniques should be\n",
    "    # considered when deciding on the patience value.\n",
    "\n",
    "    # 7. Experimentation and Cross-Validation: As with many hyperparameters in machine learning, finding the optimal\n",
    "    # patience value can require experimentation. Cross-validation can be used to test different values and find the\n",
    "    # one that offers the best balance between training time and model performance.\n",
    "\n",
    "    # In summary, there isn't a one-size-fits-all number for patience in early stopping. It often requires\n",
    "    # experimentation and consideration of your specific model and data characteristics. The goal is to find a balance\n",
    "    # between sufficient training to capture underlying patterns in the data and avoiding overfitting by training too\n",
    "    # long.\n",
    "\n",
    "    # Create an EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        # This parameter specifies the metric to be monitored. In this case, it's accuracy, which is the accuracy on\n",
    "        # the validation dataset. The callback will monitor the accuracy at the end of each epoch.\n",
    "        monitor='accuracy',\n",
    "        #  Patience is the number of epochs with no improvement after which training will be stopped. Setting patience=3\n",
    "        #  means the training process will continue for 3 more epochs even after detecting a stop in improvement. This\n",
    "        #  allowance is beneficial to rule out the possibilities of random fluctuations in training metrics.\n",
    "        patience=3,\n",
    "        # When set to True, the model weights will be restored to the weights of the epoch with the best value of the\n",
    "        # monitored metric. This ensures that even if the model's performance degrades for a few epochs before training\n",
    "        # is stopped, you will retain the best-performing version of the model.\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # The ModelCheckpoint callback is used to save the Keras model or model weights at some frequency during training.\n",
    "    # It allows you to save only the best performing model according to the performance of a specified metric.\n",
    "\n",
    "    # Benefits\n",
    "    # * Prevents Loss of a Good Model: During training, if the model's performance degrades (which can happen,\n",
    "    # especially in later epochs), this callback ensures you don't lose the best model that has been trained so far.\n",
    "    # * Saves Time: Automatically saving the best model during training eliminates the need to retrain the model to\n",
    "    # find the best version.\n",
    "    # * Convenience: Automatically handles the saving of models based on performance, making the training process more\n",
    "    # manageable and efficient.\n",
    "\n",
    "    # This callback is extremely useful when training a model for a large number of epochs.It is common in deep learning\n",
    "    # to have models that might overfit or not improve after a certain point during training.The ModelCheckpoint ensures\n",
    "    # that you always retain the best version of the model according to the chosen metric.\n",
    "\n",
    "    # Define the model checkpoint callback\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        # This parameter specifies the path where the model or weights should be saved. The file is saved in the HDF5\n",
    "        # format, which is a popular format for storing large numerical objects like weights of a neural network.\n",
    "        model_file_path,\n",
    "        # When set to True, the callback will only save the model when the monitored metric has improved, meaning the\n",
    "        # current model is better than any previous model.\n",
    "        save_best_only=True,  # Save only the best model\n",
    "        # This parameter specifies which metric to monitor. Here, it's set to monitor the mean absolute error (mae).\n",
    "        # The callback will track the changes in this metric after each epoch.\n",
    "        monitor='mae',  # Monitor the mean absolute error\n",
    "        # It is particularly useful in regression problems or situations where you want to account for the average\n",
    "        # magnitude of errors in predictions, without considering their direction.\n",
    "        #\n",
    "        # Other commonly monitored metrics include:\n",
    "        # * \"val_loss\": Monitors the loss on the validation dataset. It's useful when your primary objective is to\n",
    "        # minimize loss.\n",
    "        # * \"val_accuracy\": Tracks accuracy on the validation set. Ideal for classification problems where accuracy is\n",
    "        # the main concern.\n",
    "        # * \"loss\": Observes the model's total loss during training. This is more focused on the model's performance on\n",
    "        # the training dataset.\n",
    "        # * \"accuracy\": Similar to val_accuracy, but for the training set.\n",
    "        #\n",
    "        # Metrics for Multi-output Models: If your model has multiple outputs, Keras will add prefixes to the metric\n",
    "        # names based on the output names.\n",
    "        mode='min'  # The lower the MAE, the better\n",
    "        # Other types of mode\n",
    "        # * \"auto\": Automatically infers from the name of the monitored quantity. For instance, it sets to \"max\" for\n",
    "        # metrics like \"accuracy\", and to \"min\" for metrics like \"loss\".\n",
    "        # * \"max\": The model is saved when the monitored quantity increases. This mode is suitable for metrics where a\n",
    "        # higher value indicates better performance, such as \"accuracy\".\n",
    "    )\n",
    "\n",
    "    # Train the neural network model on the training dataset:\n",
    "    model_2.fit(\n",
    "        # This is the training data you're passing to the model.\n",
    "        train_dataset,\n",
    "        # The model will go through the entire training dataset 30 times. Each pass through the entire dataset is called\n",
    "        # an epoch.\n",
    "        epochs=epochs,\n",
    "        # The test dataset is used as validation data. This means at the end of each epoch, the model's performance is\n",
    "        # evaluated on this validation dataset.\n",
    "        validation_data=test_dataset,\n",
    "        # These are the callbacks being used during training. See above declarations for more information.\n",
    "        callbacks=[early_stopping, model_checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_loss, test_accuracy, mae = model_2.evaluate(test_dataset)\n",
    "\n",
    "    # Characteristics print\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    return model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696c6bd-7e40-42d2-b4e4-2a070d49f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = create_model(model_file_path, train_images, train_labels, test_images, test_labels, 15)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a997e-c51b-45bc-8a54-2cd60b60ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best saved model\n",
    "saved_model = load_model(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d2ab9-3560-4f16-aa28-ccb629ea3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images.reshape(-1, 28, 28, 1))  # Reshape for the CNN input\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25eeafe-d248-47a8-b8b2-2c16b91da4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_image(i):\n",
    "    plt.imshow(test_images[i], cmap='gray')\n",
    "    plt.title(f\"Actual: {class_names[test_labels[i]]}, Predicted: {class_names[predicted_classes[i]]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a few test images with predictions\n",
    "for i in range(5):  # Display first 5 images and predictions\n",
    "    display_test_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eae0a4-195c-4610-a6b5-c28fbdc67a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(model_file_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4403d7-2b95-439b-be26-77279b209c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS, lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# Generate a quantized model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open(quantized_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f48f7e-d6bd-4d09-a6b8-3587e2193b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Quantized Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a4022-3231-4d8a-902b-f3fd782568c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_images is in the range [0, 255]. Normalize it if your model expects [0, 1]\n",
    "test_images_normalized = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Load the TFLite model and allocate tensors (memory for the model's tensors)\n",
    "interpreter = Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors information from the model\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Ensure the input data is reshaped to the model's expected input shape. Since the model expects [1, 28, 28],\n",
    "# we need to ensure we're feeding in data one image at a time in a loop, without an explicit channel dimension.\n",
    "num_images = test_images_normalized.shape[0]\n",
    "\n",
    "# Initialize an empty list to store predicted classes for each image\n",
    "predicted_classes = []\n",
    "\n",
    "# Loop through each image and process it individually\n",
    "for i in range(num_images):\n",
    "    # Reshape the current image to match the model's expected input shape [1, 28, 28]\n",
    "    current_image = test_images_normalized[i].reshape(1, 28, 28)\n",
    "    \n",
    "    # Set the value of the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], current_image)\n",
    "    \n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get the prediction results for the current image\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    \n",
    "    # Append the predicted class to the list of predicted classes\n",
    "    predicted_classes.append(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8b114-dbcf-49fe-becd-b63c2deb1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_image(i):\n",
    "    plt.imshow(test_images[i], cmap='gray')\n",
    "    plt.title(f\"Actual: {class_names[test_labels[i]]}, Predicted: {class_names[predicted_classes[i]]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a few test images with predictions\n",
    "for i in range(5):  # Display first 5 images and predictions\n",
    "    display_test_image(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
