{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9fcb8d-2613-415d-84d4-8b42e710907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice to have and only here as a reference until moved to its instructional home :)\n",
    "#export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn; print(nvidia.cudnn.__file__)\"))\n",
    "#export SITE_PACKAGES_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\n",
    "#export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$SITE_PACKAGES_PATH/tensorrt_libs/:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d6cf28-70a4-4875-b109-78836094c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/flaniganp/mambaforge/envs/tensorflow-exercise-2:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       2_gnu    conda-forge\n",
      "absl-py                   2.1.0                    pypi_0    pypi\n",
      "anyio                     4.3.0                    pypi_0    pypi\n",
      "argon2-cffi               23.1.0                   pypi_0    pypi\n",
      "argon2-cffi-bindings      21.2.0                   pypi_0    pypi\n",
      "arrow                     1.3.0                    pypi_0    pypi\n",
      "asttokens                 2.4.1                    pypi_0    pypi\n",
      "async-lru                 2.0.4                    pypi_0    pypi\n",
      "attrs                     23.2.0                   pypi_0    pypi\n",
      "babel                     2.14.0                   pypi_0    pypi\n",
      "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
      "bleach                    6.1.0                    pypi_0    pypi\n",
      "bzip2                     1.0.8                hd590300_5    conda-forge\n",
      "ca-certificates           2024.2.2             hbcca054_0    conda-forge\n",
      "certifi                   2024.2.2                 pypi_0    pypi\n",
      "cffi                      1.16.0                   pypi_0    pypi\n",
      "charset-normalizer        3.3.2                    pypi_0    pypi\n",
      "comm                      0.2.1                    pypi_0    pypi\n",
      "contourpy                 1.2.0                    pypi_0    pypi\n",
      "cycler                    0.12.1                   pypi_0    pypi\n",
      "debugpy                   1.8.1                    pypi_0    pypi\n",
      "decorator                 5.1.1                    pypi_0    pypi\n",
      "defusedxml                0.7.1                    pypi_0    pypi\n",
      "exceptiongroup            1.2.0                    pypi_0    pypi\n",
      "executing                 2.0.1                    pypi_0    pypi\n",
      "fastjsonschema            2.19.1                   pypi_0    pypi\n",
      "fonttools                 4.49.0                   pypi_0    pypi\n",
      "fqdn                      1.5.1                    pypi_0    pypi\n",
      "gast                      0.5.4                    pypi_0    pypi\n",
      "google-auth               2.28.1                   pypi_0    pypi\n",
      "grpcio                    1.62.0                   pypi_0    pypi\n",
      "h11                       0.14.0                   pypi_0    pypi\n",
      "httpcore                  1.0.4                    pypi_0    pypi\n",
      "httpx                     0.27.0                   pypi_0    pypi\n",
      "idna                      3.6                      pypi_0    pypi\n",
      "ipykernel                 6.29.3                   pypi_0    pypi\n",
      "ipython                   8.22.1                   pypi_0    pypi\n",
      "ipywidgets                8.1.2                    pypi_0    pypi\n",
      "isoduration               20.11.0                  pypi_0    pypi\n",
      "jedi                      0.19.1                   pypi_0    pypi\n",
      "jinja2                    3.1.3                    pypi_0    pypi\n",
      "json5                     0.9.17                   pypi_0    pypi\n",
      "jsonpointer               2.4                      pypi_0    pypi\n",
      "jsonschema                4.21.1                   pypi_0    pypi\n",
      "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
      "jupyter                   1.0.0                    pypi_0    pypi\n",
      "jupyter-client            8.6.0                    pypi_0    pypi\n",
      "jupyter-console           6.6.3                    pypi_0    pypi\n",
      "jupyter-core              5.7.1                    pypi_0    pypi\n",
      "jupyter-events            0.9.0                    pypi_0    pypi\n",
      "jupyter-lsp               2.2.3                    pypi_0    pypi\n",
      "jupyter-server            2.12.5                   pypi_0    pypi\n",
      "jupyter-server-terminals  0.5.2                    pypi_0    pypi\n",
      "jupyterlab                4.1.2                    pypi_0    pypi\n",
      "jupyterlab-pygments       0.3.0                    pypi_0    pypi\n",
      "jupyterlab-server         2.25.3                   pypi_0    pypi\n",
      "jupyterlab-widgets        3.0.10                   pypi_0    pypi\n",
      "keras                     2.15.0                   pypi_0    pypi\n",
      "kiwisolver                1.4.5                    pypi_0    pypi\n",
      "ld_impl_linux-64          2.40                 h41732ed_0    conda-forge\n",
      "libffi                    3.4.2                h7f98852_5    conda-forge\n",
      "libgcc-ng                 13.2.0               h807b86a_5    conda-forge\n",
      "libgomp                   13.2.0               h807b86a_5    conda-forge\n",
      "libnsl                    2.0.1                hd590300_0    conda-forge\n",
      "libsqlite                 3.45.1               h2797004_0    conda-forge\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
      "libzlib                   1.2.13               hd590300_5    conda-forge\n",
      "matplotlib                3.8.3                    pypi_0    pypi\n",
      "matplotlib-inline         0.1.6                    pypi_0    pypi\n",
      "mistune                   3.0.2                    pypi_0    pypi\n",
      "ml-dtypes                 0.2.0                    pypi_0    pypi\n",
      "nbclient                  0.9.0                    pypi_0    pypi\n",
      "nbconvert                 7.16.1                   pypi_0    pypi\n",
      "nbformat                  5.9.2                    pypi_0    pypi\n",
      "ncurses                   6.4                  h59595ed_2    conda-forge\n",
      "nest-asyncio              1.6.0                    pypi_0    pypi\n",
      "notebook                  7.1.1                    pypi_0    pypi\n",
      "notebook-shim             0.2.4                    pypi_0    pypi\n",
      "numpy                     1.26.4                   pypi_0    pypi\n",
      "nvidia-cublas-cu12        12.2.5.6                 pypi_0    pypi\n",
      "nvidia-cuda-cupti-cu12    12.2.142                 pypi_0    pypi\n",
      "nvidia-cuda-nvcc-cu12     12.2.140                 pypi_0    pypi\n",
      "nvidia-cuda-nvrtc-cu12    12.2.140                 pypi_0    pypi\n",
      "nvidia-cuda-runtime-cu12  12.2.140                 pypi_0    pypi\n",
      "nvidia-cudnn-cu12         8.9.4.25                 pypi_0    pypi\n",
      "nvidia-cufft-cu12         11.0.8.103               pypi_0    pypi\n",
      "nvidia-curand-cu12        10.3.3.141               pypi_0    pypi\n",
      "nvidia-cusolver-cu12      11.5.2.141               pypi_0    pypi\n",
      "nvidia-cusparse-cu12      12.1.2.141               pypi_0    pypi\n",
      "nvidia-nccl-cu12          2.16.5                   pypi_0    pypi\n",
      "nvidia-nvjitlink-cu12     12.2.140                 pypi_0    pypi\n",
      "oauthlib                  3.2.2                    pypi_0    pypi\n",
      "openssl                   3.2.1                hd590300_0    conda-forge\n",
      "overrides                 7.7.0                    pypi_0    pypi\n",
      "pandocfilters             1.5.1                    pypi_0    pypi\n",
      "parso                     0.8.3                    pypi_0    pypi\n",
      "pexpect                   4.9.0                    pypi_0    pypi\n",
      "pillow                    10.2.0                   pypi_0    pypi\n",
      "pip                       24.0               pyhd8ed1ab_0    conda-forge\n",
      "platformdirs              4.2.0                    pypi_0    pypi\n",
      "prometheus-client         0.20.0                   pypi_0    pypi\n",
      "prompt-toolkit            3.0.43                   pypi_0    pypi\n",
      "protobuf                  4.25.3                   pypi_0    pypi\n",
      "psutil                    5.9.8                    pypi_0    pypi\n",
      "ptyprocess                0.7.0                    pypi_0    pypi\n",
      "pure-eval                 0.2.2                    pypi_0    pypi\n",
      "pyasn1                    0.5.1                    pypi_0    pypi\n",
      "pycparser                 2.21                     pypi_0    pypi\n",
      "pygments                  2.17.2                   pypi_0    pypi\n",
      "pyparsing                 3.1.1                    pypi_0    pypi\n",
      "python                    3.10.0          h543edf9_3_cpython    conda-forge\n",
      "python-dateutil           2.8.2                    pypi_0    pypi\n",
      "python-json-logger        2.0.7                    pypi_0    pypi\n",
      "pyyaml                    6.0.1                    pypi_0    pypi\n",
      "pyzmq                     25.1.2                   pypi_0    pypi\n",
      "qtconsole                 5.5.1                    pypi_0    pypi\n",
      "qtpy                      2.4.1                    pypi_0    pypi\n",
      "readline                  8.2                  h8228510_1    conda-forge\n",
      "referencing               0.33.0                   pypi_0    pypi\n",
      "requests                  2.31.0                   pypi_0    pypi\n",
      "rfc3339-validator         0.1.4                    pypi_0    pypi\n",
      "rfc3986-validator         0.1.1                    pypi_0    pypi\n",
      "rpds-py                   0.18.0                   pypi_0    pypi\n",
      "send2trash                1.8.2                    pypi_0    pypi\n",
      "setuptools                69.1.1             pyhd8ed1ab_0    conda-forge\n",
      "six                       1.16.0                   pypi_0    pypi\n",
      "sniffio                   1.3.1                    pypi_0    pypi\n",
      "soupsieve                 2.5                      pypi_0    pypi\n",
      "sqlite                    3.45.1               h2c6b66d_0    conda-forge\n",
      "stack-data                0.6.3                    pypi_0    pypi\n",
      "tensorboard               2.15.2                   pypi_0    pypi\n",
      "tensorflow                2.15.0.post1             pypi_0    pypi\n",
      "tensorflow-estimator      2.15.0                   pypi_0    pypi\n",
      "terminado                 0.18.0                   pypi_0    pypi\n",
      "tinycss2                  1.2.1                    pypi_0    pypi\n",
      "tk                        8.6.13          noxft_h4845f30_101    conda-forge\n",
      "tomli                     2.0.1                    pypi_0    pypi\n",
      "tornado                   6.4                      pypi_0    pypi\n",
      "traitlets                 5.14.1                   pypi_0    pypi\n",
      "types-python-dateutil     2.8.19.20240106          pypi_0    pypi\n",
      "typing-extensions         4.10.0                   pypi_0    pypi\n",
      "tzdata                    2024a                h0c530f3_0    conda-forge\n",
      "uri-template              1.3.0                    pypi_0    pypi\n",
      "urllib3                   2.2.1                    pypi_0    pypi\n",
      "wcwidth                   0.2.13                   pypi_0    pypi\n",
      "webcolors                 1.13                     pypi_0    pypi\n",
      "webencodings              0.5.1                    pypi_0    pypi\n",
      "websocket-client          1.7.0                    pypi_0    pypi\n",
      "wheel                     0.42.0             pyhd8ed1ab_0    conda-forge\n",
      "widgetsnbextension        4.0.10                   pypi_0    pypi\n",
      "wrapt                     1.14.1                   pypi_0    pypi\n",
      "xz                        5.2.6                h166bdaf_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250fbfae-40f2-4b22-8698-deeed76317a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:13:50.645969: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 21:13:50.664572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 21:13:50.664585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 21:13:50.665049: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 21:13:50.668404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 21:13:51.013840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Comment: This is a more extensive write-up of the much faster exercise_2_old in archive. It is mean to begin learning\n",
    "# the ins and outs of training and evaluating a tensorflow model workflow.\n",
    "\n",
    "# The os module in Python provides a way of using operating system dependent functionality. It allows you to interface\n",
    "# with the underlying operating system that Python is running on – be it Windows, Mac or Linux. You can use the os module\n",
    "# to handle file and directory paths, create folders, list contents of a directory, manage environment variables, execute\n",
    "# shell commands, and more.\n",
    "import os\n",
    "\n",
    "# The 'matplotlib.pyplot' is a collection of functions in the 'matplotlib' library that make matplotlib work like MATLAB.\n",
    "# Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure,\n",
    "# plots some lines in a plotting area, decorates the plot with labels, etc. 'plt' is a commonly used shorthand alias\n",
    "# for 'matplotlib.pyplot'. This allows you to access matplotlib's plotting functions with shorter syntax - for example,\n",
    "# you can type 'plt.plot()' instead of 'matplotlib.pyplot.plot()'. This import is essential for data visualization,\n",
    "# allowing you to create a wide variety of static, animated, and interactive plots and charts in Python.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy (Numerical Python) is an essential library for scientific computing in Python. It provides support for large,\n",
    "# multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "# With NumPy, you can perform complex mathematical operations on large amounts of data with ease and efficiency. It's\n",
    "# widely used in data analysis, machine learning, and engineering for its speed and versatility. Importing it as 'np'\n",
    "# is a common convention, making the code more readable and easier to write for the Python community.\n",
    "import numpy as np\n",
    "\n",
    "# A standard Python module for generating random numbers.\n",
    "from random import randint\n",
    "\n",
    "# TensorFlow is an open-source machine learning library developed by Google. It's used for both research and production\n",
    "# at Google.\n",
    "# * data: primarily used for data preprocessing and pipeline building. It offers tools for reading and writing data in\n",
    "#   various formats, transforming it, and making it ready for machine learning models. Efficient data handling is\n",
    "#   crucial in machine learning workflows, and TensorFlow's data module simplifies this process significantly.\n",
    "from tensorflow import data\n",
    "# * keras: originally an independent neural network library, now integrated into TensorFlow,\n",
    "#   simplifies the creation and training of deep learning models. Keras is known for its user-friendliness and modular\n",
    "#   approach, allowing for easy and fast prototyping. It provides high-level building blocks for developing deep\n",
    "#   learning models while still enabling users to dive into lower-level operations if needed.\n",
    "from tensorflow import keras\n",
    "# * tensorflow.python.client: Provides functionalities to query the properties of the hardware devices TensorFlow can\n",
    "#   access. Specifically, this module is often used to list and get detailed information about the system's available\n",
    "#   CPUs, GPUs, and other hardware accelerators compatible with TensorFlow.\n",
    "from tensorflow.python.client import device_lib\n",
    "# * keras.models: This module in Keras is essential for creating neural network models. It includes classes like\n",
    "#   Sequential and the Functional API for building models. The Sequential model is straightforward, allowing layers to\n",
    "#   be added in sequence, suitable for simple architectures. The Functional API, on the other hand, provides greater\n",
    "#   flexibility for creating complex models with advanced features like shared layers and multiple inputs/outputs.\n",
    "#   Both types enable comprehensive model management, including training, evaluation, and saving/loading\n",
    "#   functionalities, making them versatile for a wide range of deep learning applications.\n",
    "from keras.models import load_model, Sequential\n",
    "# * keras.callbacks: The keras.callbacks module offers a set of tools that can be applied during the training process of\n",
    "#   a model. These callbacks are used for various purposes like monitoring the model's performance in real-time, saving\n",
    "#   the model at certain intervals, early stopping when the performance plateaus, adjusting learning rates, and more.\n",
    "#   They are crucial for enhancing and controlling the training process, allowing for automated and optimized model\n",
    "#   training. Callbacks like ModelCheckpoint, EarlyStopping, TensorBoard, and ReduceLROnPlateau are commonly used for\n",
    "#   efficient model training and fine-tuning.\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Versioning sourcing\n",
    "from tensorflow import __version__ as tf_version\n",
    "\n",
    "# TensorFlow Lite provides tools and classes for converting TensorFlow models into a highly optimized format suitable\n",
    "# for deployment on mobile devices, embedded systems, or other platforms with limited computational capacity. This\n",
    "# module includes functionalities for model conversion, optimization, and inference. By importing `lite`, you gain\n",
    "# access to the TFLiteConverter class for model conversion, optimization options like quantization, and utilities for\n",
    "# running TFLite models on target devices.\n",
    "from tensorflow import lite\n",
    "\n",
    "# This is a convenient module provided by Keras that contains various datasets used for machine learning tasks.\n",
    "# These datasets are preprocessed and ready to use, making them ideal for educational purposes, benchmarking, and quick\n",
    "# prototyping. In the context of your code, datasets is used to access the Fashion MNIST dataset, which is a collection\n",
    "# of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset is a\n",
    "# more complex, yet similar, alternative to the classic MNIST dataset of handwritten digits.\n",
    "from keras import datasets, layers\n",
    "\n",
    "# Mixed Precision: By combining float16 and float32 data types in your model, you can speed up training and reduce\n",
    "# memory usage while maintaining the model's accuracy. This is especially beneficial when training large models or\n",
    "# working with large datasets.\n",
    "from keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Regular Expressions\n",
    "# 1. search: This function is used to perform a search for a pattern in a string and returns a match object if the\n",
    "# pattern is found, otherwise None. It's particularly useful for string pattern matching and extracting specific\n",
    "# segments from text.\n",
    "from re import search\n",
    "\n",
    "# Key aspects of 'check_output':\n",
    "# 1. **Process Execution**: The 'check_output' function is used to run a command in the subprocess/external process and\n",
    "#    capture its output. This is especially useful for running system commands and capturing their output directly\n",
    "#    within a Python script.\n",
    "# 2. **Return Output**: It returns the output of the command, making it available to the Python environment. If the\n",
    "#    called command results in an error (non-zero exit status), it raises a CalledProcessError.\n",
    "# 3. **Use Cases**: Common use cases include executing a shell command, reading the output of a command, automating\n",
    "#    scripts that interact with the command line, and integrating external tools into a Python workflow.\n",
    "# Example Usage:\n",
    "# Suppose you want to capture the output of the 'ls' command in a Unix/Linux system. You can use 'check_output' like\n",
    "# this:\n",
    "# output = check_output(['ls', '-l'])\n",
    "from subprocess import check_output\n",
    "# Key aspects of 'CalledProcessError':\n",
    "#  1. Error Handling: CalledProcessError is an exception raised by check_output when the command it tries to execute\n",
    "#   returns a non-zero exit status, indicating failure. This exception is particularly useful for error handling in\n",
    "#   scripts where the success of an external command is crucial.\n",
    "#  2. Exception Details: The exception object contains information about the error, including the return code, command\n",
    "#  executed, and output (if any). This aids in debugging by providing clear insights into why the external command\n",
    "#  failed.\n",
    "#  3. Handling the Exception: In practical use, it is often caught in a try-except block, allowing the script to respond\n",
    "#  appropriately to the failure of the external command, like logging the error or trying a fallback operation.\n",
    "from subprocess import CalledProcessError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948b393f-78aa-4f8b-8547-27593ce99fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function `print_gpu_info` is designed to display detailed information about the available GPUs on the system.\n",
    "# It utilizes TensorFlow's `device_lib.list_local_devices()` method to enumerate all computing devices recognized by\n",
    "# TensorFlow. For each device identified as a GPU, the function extracts and prints relevant details including the GPU's\n",
    "# ID, name, memory limit (converted to megabytes), and compute capability. The extraction of GPU information involves\n",
    "# parsing the device's description string using regular expressions to find specific pieces of information. This\n",
    "# function can be particularly useful for debugging or for setting up configurations in environments with multiple GPUs,\n",
    "# ensuring that TensorFlow is utilizing the GPUs as expected.\n",
    "\n",
    "def print_gpu_info():\n",
    "    # Undocumented Method\n",
    "    # https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    # Get the list of all devices\n",
    "    devices = device_lib.list_local_devices()\n",
    "\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            # Extract the physical device description\n",
    "            desc = device.physical_device_desc\n",
    "\n",
    "            # Use regular expressions to extract the required information\n",
    "            gpu_id_match = search(r'device: (\\d+)', desc)\n",
    "            name_match = search(r'name: (.*?),', desc)\n",
    "            compute_capability_match = search(r'compute capability: (\\d+\\.\\d+)', desc)\n",
    "\n",
    "            if gpu_id_match and name_match and compute_capability_match:\n",
    "                gpu_id = gpu_id_match.group(1)\n",
    "                gpu_name = name_match.group(1)\n",
    "                compute_capability = compute_capability_match.group(1)\n",
    "\n",
    "                # Convert memory limit from bytes to gigabytes and round it\n",
    "                memory_limit_gb = round(device.memory_limit / (1024 ** 2))\n",
    "\n",
    "                print(\n",
    "                    f\"\\tGPU ID {gpu_id} --> {gpu_name} --> \"\n",
    "                    f\"Memory Limit {memory_limit_gb} MB --> \"\n",
    "                    f\"Compute Capability {compute_capability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be7796a-231e-4f75-8bb8-0a3e8b6f9ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA Driver: 545.23.08\n",
      "Maximum Supported CUDA Version: 12.3     \n"
     ]
    }
   ],
   "source": [
    "# NVIDIA Driver\n",
    "try:\n",
    "    # Execute the nvidia-smi command and decode the output\n",
    "    nvidia_smi_output = check_output(\"nvidia-smi\", shell=True).decode()\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = nvidia_smi_output.split('\\n')\n",
    "\n",
    "    # Find the line containing the driver version\n",
    "    driver_line = next((line for line in lines if \"Driver Version\" in line), None)\n",
    "\n",
    "    # Extract the driver version number\n",
    "    if driver_line:\n",
    "        driver_version = driver_line.split('Driver Version: ')[1].split()[0]\n",
    "        print(\"NVIDIA Driver:\", driver_version)\n",
    "\n",
    "        # Extract the maximum supported CUDA version\n",
    "        cuda_version = driver_line.split('CUDA Version: ')[1].strip().replace(\"|\", \"\")\n",
    "        print(\"Maximum Supported CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"NVIDIA Driver Version or CUDA Version not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching NVIDIA Driver Version or CUDA Version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ce4cc0-364c-4b85-9dcd-64505b79a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Versions:\n",
      "CUDA Version 11.8.89\n"
     ]
    }
   ],
   "source": [
    "print(\"Software Versions:\")\n",
    "\n",
    "# CUDA\n",
    "try:\n",
    "    # Execute the 'nvcc --version' command and decode the output\n",
    "    nvcc_output = check_output(\"nvcc --version\", shell=True).decode()\n",
    "\n",
    "    # Use regular expression to find the version number\n",
    "    match = search(r\"V(\\d+\\.\\d+\\.\\d+)\", nvcc_output)\n",
    "    if match:\n",
    "        cuda_version = match.group(1)\n",
    "        print(\"CUDA Version\", cuda_version)\n",
    "    else:\n",
    "        print(\"CUDA Version not found\")\n",
    "\n",
    "except CalledProcessError as e:\n",
    "    print(\"Error executing nvcc --version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061669c3-5c5f-467f-9b69-69b4bd046781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:13:51.361886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.393742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.393829: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.394017: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# Sets the global dtype policy to the specified policy. In your code, set_global_policy('mixed_float16') is used to set\n",
    "# the global policy to 'mixed_float16', which means that layers with compute dtype float16 and variable dtype float32\n",
    "# will be created by default. This can lead to significant performance gains, particularly on modern GPUs.\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31dc8cb1-6a5a-4809-baa5-ec18859c3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names for Fashion MNIST\n",
    "class_names = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "# Get the key length of the dictionary of Fashion MNIST\n",
    "num_classes = len(class_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8918e773-2e9e-4946-beb4-a1283d466565",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path='../models/exercise_2.h5'\n",
    "quantized_model_path='../models/exercise_2.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa362c0-b182-45e3-ba3f-c53a4dac833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST dataset, which is a collection of 60,000 28x28 grayscale images of 10 fashion categories, along with\n",
    "# a test set of 10,000 images.\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c3c783-3e70-478c-97fe-3a9ccf647ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_file_path, train_images, train_labels, test_images, test_labels, epochs):\n",
    "    # What is One-Hot Encoding?\n",
    "    # One-hot encoding is a method to convert categorical variables into a numerical form. This process is essential for\n",
    "    # preparing categorical data for many kinds of machine learning algorithms, which require numerical input.\n",
    "\n",
    "    # How Does One-Hot Encoding Work?\n",
    "    # In one-hot encoding, each categorical value is converted into a binary vector. This vector has one 'hot' or\n",
    "    # 'active' state (represented by 1) and the rest 'cold' states (represented by 0s). The length of this vector is\n",
    "    # equal to the number of categories in the dataset.\n",
    "\n",
    "    # Why Use One-Hot Encoding?\n",
    "    # Many machine learning algorithms cannot work directly with categorical data. One-hot encoding converts the\n",
    "    # categories into a form that these algorithms can understand and process.\n",
    "\n",
    "    # Application in Your Code:\n",
    "    # Here, one-hot encoding transforms these labels into a binary matrix representation. For instance, a label '2' will\n",
    "    # be converted to [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] in a 10-dimensional space (since there are 10 categories).\n",
    "\n",
    "    # Convert the labels to one-hot encoding\n",
    "    train_labels_one_hot = keras.utils.to_categorical(train_labels, num_classes)\n",
    "    test_labels_one_hot = keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "    # What is AUTOTUNE?\n",
    "    # AUTOTUNE is a special value in TensorFlow's tf.data API. It's used to dynamically adjust the performance-related\n",
    "    # configuration settings in the data loading pipeline. Essentially, it allows TensorFlow to automatically determine\n",
    "    # the optimal number of elements to prefetch or other optimization parameters based on the available CPU and GPU\n",
    "    # resources.\n",
    "    # Why Use AUTOTUNE?\n",
    "\n",
    "    # The main goal of AUTOTUNE is to optimize the data pipeline's performance. In machine learning, especially with\n",
    "    # large datasets or complex models, data loading can become a bottleneck. AUTOTUNE helps to alleviate this by\n",
    "    # optimizing the data processing steps, leading to faster and more efficient training.\n",
    "    # How Does AUTOTUNE Work?\n",
    "\n",
    "    # When AUTOTUNE is set, TensorFlow monitors the data pipeline during runtime and adjusts the buffer sizes and other\n",
    "    # settings for operations like prefetching, caching, and shuffling. This dynamic adjustment is based on the current\n",
    "    # system's resource availability and workload, aiming to optimize throughput and reduce latency.\n",
    "\n",
    "    # Advantages:\n",
    "    # Improved Performance: By allowing TensorFlow to optimize prefetch buffer sizes, the I/O time decreases, and the\n",
    "    # data feeding pipeline becomes more efficient.\n",
    "    # Resource Management: It helps in better utilization of CPU and GPU resources by dynamically adjusting to the\n",
    "    # system's available resources.\n",
    "    # Ease of Use: Instead of manually tuning the performance parameters, AUTOTUNE simplifies the process, making it\n",
    "    # easier to work with complex data pipelines.\n",
    "\n",
    "    # Apply AUTOTUNE to dataset operations\n",
    "    batch_size = 32\n",
    "    train_dataset = data.Dataset.from_tensor_slices((train_images, train_labels_one_hot))\n",
    "    train_dataset = (train_dataset.\n",
    "                     cache().  # This method caches the dataset in memory. Once the dataset is loaded into memory,\n",
    "                     # subsequent iterations over the data will be much faster.\n",
    "                     shuffle(len(train_images)).  # This method randomly shuffles the elements of the dataset.\n",
    "                     # The len(train_images) argument specifies the buffer size for shuffling.\n",
    "                     batch(batch_size).  # This method combines consecutive elements of the dataset into batches.\n",
    "                     # Here, it's creating batches of 32 elements (images and labels).\n",
    "                     prefetch(data.AUTOTUNE))  # This is where AUTOTUNE is applied. Prefetching allows later elements of\n",
    "    # the dataset to be prepared while the current element is being processed. This can improve the efficiency of the\n",
    "    # pipeline. By setting it to AUTOTUNE, you're allowing TensorFlow to automatically manage the buffer size for\n",
    "    # prefetching based on runtime conditions.\n",
    "\n",
    "    # Apply AUTOTUNE to test dataset operations\n",
    "    test_dataset = data.Dataset.from_tensor_slices((test_images, test_labels_one_hot))\n",
    "    test_dataset = test_dataset.batch(batch_size).cache().prefetch(data.AUTOTUNE)\n",
    "\n",
    "    # Example: printing one-hot encoded labels for the first training sample\n",
    "    print(\"Original label: \", train_labels[0])\n",
    "    print(\"One-Hot Encoded Label: \", train_labels_one_hot[0])\n",
    "\n",
    "    # Get a random index\n",
    "    random_index = randint(0, len(train_images) - 1)\n",
    "\n",
    "    # Get the random image and its label\n",
    "    random_image = train_images[random_index]\n",
    "    random_label = train_labels[random_index]\n",
    "\n",
    "    # Print the label and its corresponding class name\n",
    "    print(\"Label:\", random_label)\n",
    "    print(\"Class Name:\", class_names[random_label])\n",
    "\n",
    "    # Print the shape of the image\n",
    "    print(\"Image Shape:\", random_image.shape)\n",
    "\n",
    "    # Normalize images (get values between 0 & 1)\n",
    "    train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "    # Check the shape of the input data\n",
    "    print(f\"Print the train image shape {train_images.shape}\")\n",
    "    print(f\"print the train labels shape {train_labels.shape}\")\n",
    "\n",
    "    # Assuming the input shape is (height, width), for example, (28, 28) MNIST dataset\n",
    "    # Tuple Unpacking: In Python, the asterisk (*) is used to unpack a tuple. This means that the elements of the tuple\n",
    "    # are \"taken out\" and used individually.\n",
    "    input_shape = (28, 28)\n",
    "    # Tuple Extension: By placing the unpacked tuple and another element (in this case, 1) inside parentheses and\n",
    "    # separating them by a comma, a new tuple is created that extends the original tuple.\n",
    "    reshaped_input_shape = (*input_shape, 1)\n",
    "\n",
    "    # Build the model\n",
    "    # This is a Keras model that linearly stacks layers. It's ideal for a plain stack of layers where each layer has\n",
    "    # exactly one input tensor and one output tensor.\n",
    "\n",
    "    model_2 = Sequential([\n",
    "        # Reshape Layer\n",
    "        # Adjusts the input data shape to a format suitable for convolutional layers. It's crucial when your input data\n",
    "        # does not already have the required dimensions (like adding a channel dimension to grayscale images).\n",
    "        layers.Reshape(reshaped_input_shape, input_shape=input_shape),\n",
    "\n",
    "        # First convolutional layer\n",
    "        layers.Conv2D(\n",
    "            32,  # Number of filters (or kernels). Each filter extracts different features from the input image.\n",
    "            3,  # Kernel size (3x3). This size is a common choice for extracting features.\n",
    "            padding='same',  # Padding means the output size is the same as the input size. This is achieved by adding\n",
    "            # padding to the input.\n",
    "            activation=\"relu\"  # (Rectified Linear Unit) is a common activation function that introduces non-linearity\n",
    "            # to the model, allowing it to learn more complex patterns.\n",
    "        ),\n",
    "\n",
    "        # Pool Size: (2, 2) reduces the spatial dimensions (height and width) of the output from the previous layer.\n",
    "        # It helps in reducing the computational load and overfitting.\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        # See first comment for first convolutional layer\n",
    "        layers.Conv2D(32, 3, padding='same', activation=\"relu\"),\n",
    "\n",
    "        # Pool Size: (2, 2) reduces the spatial dimensions (height and width) of the output from the previous layer.\n",
    "        # It helps in reducing the computational load and overfitting.\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        # See first comment for first convolutional layer\n",
    "        layers.Conv2D(16, 3, padding='same', activation=\"relu\"),\n",
    "\n",
    "        # Pool Size: (2, 2) reduces the spatial dimensions (height and width) of the output from the previous layer.\n",
    "        # It helps in reducing the computational load and overfitting.\n",
    "        layers.MaxPool2D(),\n",
    "\n",
    "        # Converts the 2D output of the previous layers into a 1D array. This is necessary because the next dense layer\n",
    "        # expects a 1D input.\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dense(\n",
    "            num_classes,  # Number of neurons. This should be equal to the number of classes in the classification task.\n",
    "            activation=\"softmax\",  # \"softmax\" is used for multi-class classification. It outputs a probability\n",
    "            # distribution over the classes.\n",
    "            dtype='float32'  # 'float32'. This is explicitly set due to mixed precision training (combining float16 and\n",
    "            # float32 for performance).\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # The compile method of a Keras model is where you configure the learning process before training the model. This\n",
    "    # method is essential as it sets up the optimizer, loss function, and metrics to be used for training and evaluating\n",
    "    # the model. Let's dissect each component of the compile method in your code:\n",
    "    model_2.compile(\n",
    "        # categorical_crossentropy: This is the loss function used for multi-class classification problems where labels\n",
    "        # are provided in a one-hot encoded format. The categorical cross-entropy loss compares the distribution of the\n",
    "        # predictions (the outputs of the softmax in your model) with the true distribution (the true labels of the data\n",
    "        # ). In simpler terms, this function calculates the difference between the actual labels and the predicted\n",
    "        # labels. The goal of training is to minimize this difference.\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        # Adam Optimizer: Adam is an optimization algorithm that can be used instead of the classical stochastic\n",
    "        # gradient descent procedure to update network weights iteratively based on training data.\n",
    "\n",
    "        # Learning Rate (0.001): This is one of the most important hyperparameters in training neural networks. The\n",
    "        # learning rate defines the step size at each iteration while moving toward a minimum of the loss function. A\n",
    "        # learning rate of 0.001 is a good starting point for many models.\n",
    "\n",
    "        # Adam optimizer combines the advantages of two other extensions of stochastic gradient descent, namely Adaptive\n",
    "        # Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It's known for its efficiency and\n",
    "        # effectiveness in practice, especially in situations with large datasets or parameters.\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "\n",
    "        # 1. Accuracy\n",
    "        # * Usage: Widely used in classification problems.\n",
    "        #\n",
    "        # * Explanation: Accuracy is the most intuitive performance measure and it is simply a ratio of correctly\n",
    "        # predicted observations to the total observations. It's a great measure when the target classes are well\n",
    "        # balanced.\n",
    "        #\n",
    "        # * Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN), where TP = True Positives, TN = True Negatives,\n",
    "        # FP = False Positives, and FN = False Negatives.\n",
    "        #\n",
    "        # * Considerations: While accuracy is very intuitive, it can be misleading in cases where the class distribution\n",
    "        # is imbalanced. For example, in a dataset with 95% of Class A and 5% of Class B, a model that always predicts\n",
    "        # Class A will still have a high accuracy of 95%, despite not being useful.\n",
    "\n",
    "        # 2. Mean Absolute Error (MAE)\n",
    "        # * Usage: Commonly used in regression problems but can be adapted for classification.\n",
    "        #\n",
    "        # * Explanation: MAE measures the average magnitude of the errors in a set of predictions, without considering\n",
    "        # their direction. It’s the average over the test sample of the absolute differences between prediction and\n",
    "        # actual observation where all individual differences have equal weight.\n",
    "        #\n",
    "        # * Formula: MAE = (1/n) * Σ|actual - forecast|, where n is the number of observations.\n",
    "        #\n",
    "        # * Considerations: MAE is a linear score, which means all the individual differences are weighted equally in\n",
    "        # the average. For example, in a temperature forecasting model, an MAE of 2 degrees means the average prediction\n",
    "        # was within 2 degrees of the actual temperature. In classification, it can be used to indicate how far off the\n",
    "        # predictions are on average, but it's less common and less intuitive than in regression.\n",
    "        metrics=[\"accuracy\", \"mae\"]\n",
    "\n",
    "        # Other Metrics to consider:\n",
    "\n",
    "        # 1. Precision\n",
    "        # * Usage: Particularly useful in scenarios where False Positives are more concerning than False Negatives.\n",
    "        #\n",
    "        # * Explanation: Precision is the ratio of correctly predicted positive observations to the total predicted\n",
    "        # positives. High precision relates to a low rate of false positives.\n",
    "        #\n",
    "        # * Formula: Precision = TP / (TP + FP), where TP is True Positives and FP is False Positives.\n",
    "        #\n",
    "        # * Consideration: Precision becomes especially important in scenarios where False Positives are costly.\n",
    "        # For example, in email spam detection, a false positive (marking a legitimate email as spam) can be more\n",
    "        # problematic than a false negative (failing to mark a spam email). However, focusing only on precision might\n",
    "        # lead to a model that is overly cautious, potentially missing out on many true positive cases.\n",
    "\n",
    "        # 2. Recall (Sensitivity)\n",
    "        # * Usage: Important in cases where False Negatives are more critical than False Positives.\n",
    "        #\n",
    "        # * Explanation: Recall is the ratio of correctly predicted positive observations to all the observations in the\n",
    "        # actual class. High recall relates to a low rate of false negatives.\n",
    "        #\n",
    "        # * Formula: Recall = TP / (TP + FN), where FN is False Negatives.\n",
    "        #\n",
    "        # * Consideration: Recall should be prioritized in situations where missing a True Positive is more critical\n",
    "        # than incorrectly labeling a False Positive. For example, in medical testing for a serious disease, it's\n",
    "        # crucial to identify all potential cases (high recall), even at the risk of some false alarms\n",
    "        # (lower precision). However, a high recall with very low precision can lead to many false alarms, which might\n",
    "        # be costly or inefficient.\n",
    "\n",
    "        # 3. F1 Score\n",
    "        # * Usage: Useful when you want to balance precision and recall.\n",
    "        #\n",
    "        # * Explanation: The F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both\n",
    "        # false positives and false negatives into account.\n",
    "        #\n",
    "        # * Formula: F1 Score = 2*(Recall * Precision) / (Recall + Precision).\n",
    "        #\n",
    "        # * Consideration: F1 Score is useful when seeking a balance between Precision and Recall, especially in cases\n",
    "        # where an uneven class distribution makes accuracy a less reliable metric. However, the F1 Score treats\n",
    "        # precision and recall as equally important, which might not be suitable for all contexts. For instance, in\n",
    "        # certain scenarios, recall might be significantly more important than precision and vice versa.\n",
    "\n",
    "        # 4. Area Under the Curve (AUC) [BINARY CLASSICATION]\n",
    "        # * Usage: Commonly used in binary classification problems.\n",
    "        #\n",
    "        # * Explanation: AUC represents the ability of the model to distinguish between classes. An AUC of 0.5 suggests\n",
    "        # no discrimination ability, whereas an AUC of 1.0 represents perfect discrimination.\n",
    "        #\n",
    "        # * Consideration: AUC is a powerful metric for binary classification problems, as it provides an aggregate\n",
    "        # measure of performance across all possible classification thresholds. However, AUC doesn't convey information\n",
    "        # about the actual values of thresholds and how they affect the balance between false positives and false\n",
    "        # negatives. It's also less informative in multi-class classification scenarios.\n",
    "\n",
    "        # 5. Mean Squared Error (MSE)\n",
    "        # * Usage: Common in regression problems.\n",
    "        #\n",
    "        # * Explanation: MSE measures the average of the squares of the errors—that is, the average squared difference\n",
    "        # between the estimated values and the actual value.\n",
    "        #\n",
    "        # * Formula: MSE = (1/n) * Σ(actual - forecast)², where n is the number of observations.\n",
    "        #\n",
    "        # * Consideration: MSE is sensitive to outliers as it squares the errors before averaging them. This can lead to\n",
    "        # situations where the model is heavily penalized for a small number of large errors, which might not be\n",
    "        # desirable in all contexts. MSE is more suitable when large errors are particularly undesirable and need more\n",
    "        # attention.\n",
    "\n",
    "        # 6. Root Mean Squared Error (RMSE)\n",
    "        # * Usage: Also common in regression problems.\n",
    "        #\n",
    "        # * Explanation: RMSE is the square root of the mean of the squared errors. It's a measure of how spread out\n",
    "        # these residuals are, i.e., it tells you how concentrated the data is around the line of best fit.\n",
    "        #\n",
    "        # * Formula: RMSE = √[Σ(predicted - actual)² / n].\n",
    "        #\n",
    "        # * Consideration: Similar to MSE, RMSE also gives higher weight to larger errors (due to squaring). This can be\n",
    "        # beneficial when large errors are more significant, but it can also skew the model's focus towards larger\n",
    "        # errors. RMSE can be more interpretable than MSE as it is in the same units as the response variable.\n",
    "\n",
    "        # 7. Logarithmic Loss (Log Loss)\n",
    "        #\n",
    "        # * Usage: Common in binary and multi-class classification problems.\n",
    "        #\n",
    "        # * Explanation: Log loss measures the performance of a classification model where the prediction is a\n",
    "        # probability value between 0 and 1. Log loss increases as the predicted probability diverge from the actual\n",
    "        # label.\n",
    "        #\n",
    "        # * Consideration: Log Loss is sensitive to the predicted probabilities and is a strict measure, heavily\n",
    "        # penalizing confident but wrong predictions. While it's a good measure for evaluating probabilistic outputs,\n",
    "        # its strictness can sometimes lead to misleading conclusions, especially if the model is calibrated to be\n",
    "        # overly confident.\n",
    "    )\n",
    "\n",
    "    # The EarlyStopping callback in Keras is used to stop training the model when a monitored metric has stopped\n",
    "    # improving. This is a form of regularization used to prevent overfitting and to ensure that the model doesn't waste\n",
    "    # computational resources by continuing to train past the point of diminishing returns.\n",
    "\n",
    "    # Benefits\n",
    "    # * Prevents Overfitting: By stopping training once the model performance ceases to improve, EarlyStopping helps to\n",
    "    # prevent the model from overfitting to the training data.\n",
    "    # * Saves Time and Resources: It reduces unnecessary computation and resource usage by cutting short the training\n",
    "    # process when further training is unlikely to improve the model's performance.\n",
    "    # * Optimizes Model Performance: With the restore_best_weights option, the model is guaranteed to exit training with\n",
    "    # the most effective weights it found, rather than potentially overfit weights from the final epochs of training.\n",
    "\n",
    "    # Usage Scenario\n",
    "    # The EarlyStopping callback is particularly useful in scenarios where you don't know beforehand how many epochs are\n",
    "    # needed to train the model. Instead of arbitrarily choosing a large number of epochs and risking overfitting,\n",
    "    # EarlyStopping allows the model to determine the optimal number of epochs based on the actual training dynamics.\n",
    "    #\n",
    "    # It's a smart way to ensure efficient and effective training, especially when working with large and complex\n",
    "    # datasets.\n",
    "\n",
    "    # Best Practices:\n",
    "    # Determining the right value for the patience parameter in EarlyStopping can be somewhat nuanced, as it depends on\n",
    "    # the specific characteristics of your training data and model. However, there are general guidelines and best\n",
    "    # practices you can consider:\n",
    "\n",
    "    # 1. Change in Performance Metric: A standard approach is to monitor a specific performance metric, like validation\n",
    "    # accuracy or loss, and stop training if there's no significant change in this metric over a given number of epochs.\n",
    "    # What constitutes a \"significant change\" can vary; sometimes a minimal improvement threshold (like 1% in accuracy)\n",
    "    # is set to avoid stopping due to minor fluctuations.\n",
    "\n",
    "    # 2. Generalization vs. Training Time: Research suggests that \"slower\" stopping criteria, which allow more epochs\n",
    "    # before stopping, can lead to improved generalization (better performance on unseen data) compared to \"faster\"\n",
    "    # criteria. However, this comes at the cost of longer training times and increased computational resources.\n",
    "\n",
    "    # 3. Monitoring Loss vs. Other Metrics: While it's common to monitor loss, it may not always capture what's most\n",
    "    # important about your model. You might want to choose a performance metric that best defines the model's utility\n",
    "    # for its intended application. For example, in a classification task, you might prioritize accuracy or F1 score\n",
    "    # over loss.\n",
    "\n",
    "    # 4. Learning Curves Analysis: Before implementing early stopping, it's helpful to train your model for a\n",
    "    # sufficiently long time without it and plot learning curves. This can give you insights into how noisy the training\n",
    "    #  process is and help you better decide the patience value.\n",
    "\n",
    "    # 5. Model Complexity and Dataset Size: The optimal patience value can also depend on the complexity of your model\n",
    "    # and the size of your dataset. For simpler models or smaller datasets, a smaller patience might suffice, while more\n",
    "    # complex models or larger datasets might benefit from a larger patience value to fully realize their learning\n",
    "    # potential.\n",
    "\n",
    "    # 6. Early Stopping in Conjunction with Other Techniques: Early stopping is often used in conjunction with other\n",
    "    # regularization techniques like dropout or weight decay. The combined effect of these techniques should be\n",
    "    # considered when deciding on the patience value.\n",
    "\n",
    "    # 7. Experimentation and Cross-Validation: As with many hyperparameters in machine learning, finding the optimal\n",
    "    # patience value can require experimentation. Cross-validation can be used to test different values and find the\n",
    "    # one that offers the best balance between training time and model performance.\n",
    "\n",
    "    # In summary, there isn't a one-size-fits-all number for patience in early stopping. It often requires\n",
    "    # experimentation and consideration of your specific model and data characteristics. The goal is to find a balance\n",
    "    # between sufficient training to capture underlying patterns in the data and avoiding overfitting by training too\n",
    "    # long.\n",
    "\n",
    "    # Create an EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        # This parameter specifies the metric to be monitored. In this case, it's accuracy, which is the accuracy on\n",
    "        # the validation dataset. The callback will monitor the accuracy at the end of each epoch.\n",
    "        monitor='accuracy',\n",
    "        #  Patience is the number of epochs with no improvement after which training will be stopped. Setting patience=3\n",
    "        #  means the training process will continue for 3 more epochs even after detecting a stop in improvement. This\n",
    "        #  allowance is beneficial to rule out the possibilities of random fluctuations in training metrics.\n",
    "        patience=3,\n",
    "        # When set to True, the model weights will be restored to the weights of the epoch with the best value of the\n",
    "        # monitored metric. This ensures that even if the model's performance degrades for a few epochs before training\n",
    "        # is stopped, you will retain the best-performing version of the model.\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # The ModelCheckpoint callback is used to save the Keras model or model weights at some frequency during training.\n",
    "    # It allows you to save only the best performing model according to the performance of a specified metric.\n",
    "\n",
    "    # Benefits\n",
    "    # * Prevents Loss of a Good Model: During training, if the model's performance degrades (which can happen,\n",
    "    # especially in later epochs), this callback ensures you don't lose the best model that has been trained so far.\n",
    "    # * Saves Time: Automatically saving the best model during training eliminates the need to retrain the model to\n",
    "    # find the best version.\n",
    "    # * Convenience: Automatically handles the saving of models based on performance, making the training process more\n",
    "    # manageable and efficient.\n",
    "\n",
    "    # This callback is extremely useful when training a model for a large number of epochs.It is common in deep learning\n",
    "    # to have models that might overfit or not improve after a certain point during training.The ModelCheckpoint ensures\n",
    "    # that you always retain the best version of the model according to the chosen metric.\n",
    "\n",
    "    # Define the model checkpoint callback\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        # This parameter specifies the path where the model or weights should be saved. The file is saved in the HDF5\n",
    "        # format, which is a popular format for storing large numerical objects like weights of a neural network.\n",
    "        model_file_path,\n",
    "        # When set to True, the callback will only save the model when the monitored metric has improved, meaning the\n",
    "        # current model is better than any previous model.\n",
    "        save_best_only=True,  # Save only the best model\n",
    "        # This parameter specifies which metric to monitor. Here, it's set to monitor the mean absolute error (mae).\n",
    "        # The callback will track the changes in this metric after each epoch.\n",
    "        monitor='mae',  # Monitor the mean absolute error\n",
    "        # It is particularly useful in regression problems or situations where you want to account for the average\n",
    "        # magnitude of errors in predictions, without considering their direction.\n",
    "        #\n",
    "        # Other commonly monitored metrics include:\n",
    "        # * \"val_loss\": Monitors the loss on the validation dataset. It's useful when your primary objective is to\n",
    "        # minimize loss.\n",
    "        # * \"val_accuracy\": Tracks accuracy on the validation set. Ideal for classification problems where accuracy is\n",
    "        # the main concern.\n",
    "        # * \"loss\": Observes the model's total loss during training. This is more focused on the model's performance on\n",
    "        # the training dataset.\n",
    "        # * \"accuracy\": Similar to val_accuracy, but for the training set.\n",
    "        #\n",
    "        # Metrics for Multi-output Models: If your model has multiple outputs, Keras will add prefixes to the metric\n",
    "        # names based on the output names.\n",
    "        mode='min'  # The lower the MAE, the better\n",
    "        # Other types of mode\n",
    "        # * \"auto\": Automatically infers from the name of the monitored quantity. For instance, it sets to \"max\" for\n",
    "        # metrics like \"accuracy\", and to \"min\" for metrics like \"loss\".\n",
    "        # * \"max\": The model is saved when the monitored quantity increases. This mode is suitable for metrics where a\n",
    "        # higher value indicates better performance, such as \"accuracy\".\n",
    "    )\n",
    "\n",
    "    # Train the neural network model on the training dataset:\n",
    "    model_2.fit(\n",
    "        # This is the training data you're passing to the model.\n",
    "        train_dataset,\n",
    "        # The model will go through the entire training dataset 30 times. Each pass through the entire dataset is called\n",
    "        # an epoch.\n",
    "        epochs,\n",
    "        # The test dataset is used as validation data. This means at the end of each epoch, the model's performance is\n",
    "        # evaluated on this validation dataset.\n",
    "        validation_data=test_dataset,\n",
    "        # These are the callbacks being used during training. See above declarations for more information.\n",
    "        callbacks=[early_stopping, model_checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_loss, test_accuracy, mae = model_2.evaluate(test_dataset)\n",
    "\n",
    "    # Characteristics print\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    return model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1696c6bd-7e40-42d2-b4e4-2a070d49f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label:  9\n",
      "One-Hot Encoded Label:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Label: 1\n",
      "Class Name: Trouser\n",
      "Image Shape: (28, 28)\n",
      "Print the train image shape (60000, 28, 28)\n",
      "print the train labels shape (60000,)\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:13:51.632139: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.632220: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.632260: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.676032: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.676112: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.676167: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 21:13:51.676212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22046 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-02-28 21:13:52.439770: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1875 [..............................] - ETA: 27:35 - loss: 51.3433 - accuracy: 0.0938 - mae: 0.1812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 21:13:52.888697: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f5054005fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-28 21:13:52.888713: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-02-28 21:13:52.892660: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709172832.943450   44397 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 13s 6ms/step - loss: 0.8531 - accuracy: 0.7928 - mae: 0.0537 - val_loss: 0.4468 - val_accuracy: 0.8443 - val_mae: 0.0441\n",
      "Epoch 2/30\n",
      "  44/1875 [..............................] - ETA: 6s - loss: 0.4543 - accuracy: 0.8374 - mae: 0.0449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flaniganp/mambaforge/envs/tensorflow-exercise-2/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3819 - accuracy: 0.8637 - mae: 0.0383 - val_loss: 0.4255 - val_accuracy: 0.8503 - val_mae: 0.0376\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3340 - accuracy: 0.8787 - mae: 0.0339 - val_loss: 0.3775 - val_accuracy: 0.8639 - val_mae: 0.0361\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3070 - accuracy: 0.8892 - mae: 0.0313 - val_loss: 0.3329 - val_accuracy: 0.8788 - val_mae: 0.0321\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2903 - accuracy: 0.8948 - mae: 0.0295 - val_loss: 0.3300 - val_accuracy: 0.8831 - val_mae: 0.0317\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2746 - accuracy: 0.8996 - mae: 0.0280 - val_loss: 0.3234 - val_accuracy: 0.8866 - val_mae: 0.0306\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2631 - accuracy: 0.9037 - mae: 0.0269 - val_loss: 0.3412 - val_accuracy: 0.8836 - val_mae: 0.0294\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2542 - accuracy: 0.9062 - mae: 0.0260 - val_loss: 0.3261 - val_accuracy: 0.8833 - val_mae: 0.0296\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2452 - accuracy: 0.9095 - mae: 0.0252 - val_loss: 0.3134 - val_accuracy: 0.8913 - val_mae: 0.0278\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2414 - accuracy: 0.9110 - mae: 0.0247 - val_loss: 0.3205 - val_accuracy: 0.8916 - val_mae: 0.0270\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2354 - accuracy: 0.9129 - mae: 0.0240 - val_loss: 0.3070 - val_accuracy: 0.8888 - val_mae: 0.0292\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2307 - accuracy: 0.9157 - mae: 0.0236 - val_loss: 0.3176 - val_accuracy: 0.8900 - val_mae: 0.0275\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2266 - accuracy: 0.9165 - mae: 0.0232 - val_loss: 0.3227 - val_accuracy: 0.8873 - val_mae: 0.0287\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2236 - accuracy: 0.9168 - mae: 0.0228 - val_loss: 0.3238 - val_accuracy: 0.8894 - val_mae: 0.0277\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2179 - accuracy: 0.9195 - mae: 0.0222 - val_loss: 0.3621 - val_accuracy: 0.8835 - val_mae: 0.0280\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2138 - accuracy: 0.9197 - mae: 0.0218 - val_loss: 0.3201 - val_accuracy: 0.8903 - val_mae: 0.0283\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2120 - accuracy: 0.9210 - mae: 0.0216 - val_loss: 0.3247 - val_accuracy: 0.8925 - val_mae: 0.0272\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2081 - accuracy: 0.9227 - mae: 0.0213 - val_loss: 0.3302 - val_accuracy: 0.8928 - val_mae: 0.0259\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2022 - accuracy: 0.9250 - mae: 0.0206 - val_loss: 0.3309 - val_accuracy: 0.8951 - val_mae: 0.0255\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2002 - accuracy: 0.9262 - mae: 0.0204 - val_loss: 0.3389 - val_accuracy: 0.8940 - val_mae: 0.0257\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2034 - accuracy: 0.9255 - mae: 0.0205 - val_loss: 0.3627 - val_accuracy: 0.8913 - val_mae: 0.0251\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1991 - accuracy: 0.9262 - mae: 0.0200 - val_loss: 0.3603 - val_accuracy: 0.8836 - val_mae: 0.0278\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1977 - accuracy: 0.9269 - mae: 0.0200 - val_loss: 0.3450 - val_accuracy: 0.8873 - val_mae: 0.0281\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1932 - accuracy: 0.9280 - mae: 0.0196 - val_loss: 0.3757 - val_accuracy: 0.8915 - val_mae: 0.0252\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1913 - accuracy: 0.9289 - mae: 0.0193 - val_loss: 0.3523 - val_accuracy: 0.8915 - val_mae: 0.0265\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1922 - accuracy: 0.9283 - mae: 0.0194 - val_loss: 0.3681 - val_accuracy: 0.8893 - val_mae: 0.0257\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1887 - accuracy: 0.9302 - mae: 0.0190 - val_loss: 0.3485 - val_accuracy: 0.8930 - val_mae: 0.0261\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1921 - accuracy: 0.9298 - mae: 0.0192 - val_loss: 0.3647 - val_accuracy: 0.8904 - val_mae: 0.0259\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1826 - accuracy: 0.9321 - mae: 0.0185 - val_loss: 0.3791 - val_accuracy: 0.8943 - val_mae: 0.0251\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1855 - accuracy: 0.9318 - mae: 0.0186 - val_loss: 0.3658 - val_accuracy: 0.8896 - val_mae: 0.0264\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3658 - accuracy: 0.8896 - mae: 0.0264\n",
      "Test Loss: 0.3658369779586792\n",
      "Test Accuracy: 0.8895999789237976\n",
      "Mean Absolute Error: 0.026418883353471756\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = create_model(model_file_path, train_images, train_labels, test_images, test_labels, 15)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "549a997e-c51b-45bc-8a54-2cd60b60ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Loading the best saved model\n",
    "saved_model = load_model(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451d2ab9-3560-4f16-aa28-ccb629ea3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 418us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_images.reshape(-1, 28, 28, 1))  # Reshape for the CNN input\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b25eeafe-d248-47a8-b8b2-2c16b91da4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfV0lEQVR4nO3deXBV9f3G8ecmJDfJDSEBkrBJwlJWZadQLQaqFCjLQKEqrcqO04paZ0qntlOXOlpHbetWi9qqDKZDFaKiBUELTrEuiFaEKhQwggICMQlkX8/vjw6fnzGB5PsVYgjv1wx/JDnPOeeenJvnnpuTD6EgCAIBACAp6uveAQBAy0EpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKZxDQqGQbr311mbZzpIlSxpd7sknn1QoFNLHH398xveppXj11VcVCoX06quv2ufmzp2rzMzMr22fvqyhfTyT21m1alWjy7a0Y9SaUQqeHn74YYVCIY0aNcp7HQcPHtStt96q99577/Tt2Bm0fft2zZo1SxkZGYqLi1PXrl01fvx4Pfjgg82y/TvvvFPPPffcV1rH2LFjFQqF7F/79u01cuRIPf7446qtrT09O9pMTsfxOFNeeOEFZWVlKS0tTQkJCerZs6cuu+wyvfTSS2d822fb86qloRQ8ZWdnKzMzU1u2bNGePXu81nHw4EHddtttZ8XJ+/rrr2vEiBHatm2bFi1apIceekgLFy5UVFSU7r//fq91XnXVVSorK1NGRkaTlj9dPwS7deumFStWaMWKFfr1r3+t6upqLViwQL/85S+/8rp9PPbYY9q1a5dzrqWWwr333qtp06YpFArppptu0h/+8AfNnDlTu3fv1sqVK73W6XKMzqbnVUvU5uvegbNRbm6uXn/9deXk5Oiaa65Rdna2brnllq97t86oO+64Q+3atdPbb7+t5OTkOl87cuSI1zqjo6MVHR19ymWCIFB5ebni4+O9ttGQdu3a6corr7SPr7nmGvXt21cPPfSQbr/9dsXExNTL1NbWqrKyUnFxcadtP05oaHtnq+rqat1+++0aP368NmzYUO/rvudKU45RdXX1WXe11xJxpeAhOztbKSkpmjx5smbNmqXs7OwGlyssLNSNN96ozMxMhcNhdevWTVdffbXy8vL06quvauTIkZKkefPm2dsZTz75pCQpMzNTc+fOrbfOsWPHauzYsfZxZWWlbr75Zg0fPlzt2rVTJBLRmDFjtGnTpiY9lp07d2r//v2NLrd3714NHDiwXiFIUlpaWoOZ5557Tueff77C4bAGDhxY762Dhn6nkJmZqSlTpmj9+vUaMWKE4uPj9cgjjygUCqmkpETLly+3Y9XQ8fGRkJCg0aNHq6SkREePHpX0/78Xyc7O1sCBAxUOh23/Dxw4oPnz5ys9Pd0e2+OPP15vvZ9++qmmT5+uSCSitLQ03XjjjaqoqKi3XEPvl9fW1ur+++/XBRdcoLi4OKWmpmrixInaunWr7d+pjsfp3sfS0lLt3LlTeXl5pzyWeXl5On78uC666KIGv97QuVJbW6s77rhD3bp1U1xcnC655JJ6V99fPkYff/yxQqGQ7r33Xt13333q1auXwuGwHn744VM+r9A4rhQ8ZGdn6/vf/75iY2M1e/Zs/elPf9Lbb79tJ6MkFRcXa8yYMfrwww81f/58DRs2THl5eVqzZo0+/fRT9e/fX7/5zW908803a/HixRozZowk6cILL3Tal+PHj+vPf/6zZs+erUWLFqmoqEh/+ctfNGHCBG3ZskVDhgw5Zb5///7Kyspq9JeKGRkZeuONN7Rjxw6df/75je7Xa6+9ppycHP3kJz9R27Zt9cADD2jmzJnav3+/OnTocMrsrl27NHv2bF1zzTVatGiR+vbtqxUrVmjhwoX65je/qcWLF0uSevXq1eh+NNVHH32k6OjoOqW3ceNGPf3001qyZIk6duyozMxMHT58WKNHj7bSSE1N1bp167RgwQIdP35cP/3pTyVJZWVluuSSS7R//35df/316tKli1asWKGNGzc2aX8WLFigJ598UpMmTdLChQtVXV2tzZs3680339SIESNOeTzOxD5u2bJF48aN0y233HLKmxXS0tIUHx+vF154Qdddd53at2/f6GO96667FBUVpZ/97Gc6duyY7r77bv3oRz/SW2+91Wj2iSeeUHl5uRYvXqxwOKwZM2aoqKjoKz+vzmkBnGzdujWQFLz88stBEARBbW1t0K1bt+CGG26os9zNN98cSApycnLqraO2tjYIgiB4++23A0nBE088UW+ZjIyMYM6cOfU+n5WVFWRlZdnH1dXVQUVFRZ1lCgoKgvT09GD+/Pl1Pi8puOWWW+p97ovrO5kNGzYE0dHRQXR0dPCtb30r+PnPfx6sX78+qKysrLespCA2NjbYs2ePfW7btm2BpODBBx+0zz3xxBOBpCA3N7fO45YUvPTSS/XWG4lEGjwmLrKysoJ+/foFR48eDY4ePRp8+OGHwfXXXx9ICqZOnVrnMURFRQX/+c9/6uQXLFgQdO7cOcjLy6vz+SuuuCJo165dUFpaGgRBENx3332BpODpp5+2ZUpKSoLevXsHkoJNmzbZ5+fMmRNkZGTYxxs3bgwkBddff329/T9x7gTByY/HmdjHTZs2NXj+NOTEuR+JRIJJkyYFd9xxR/DOO+/UW+7EOvv371/nHL7//vsDScH27dvtc18+Rrm5uYGkICkpKThy5Eid9Z7qeYXG8faRo+zsbKWnp2vcuHGS/ncZf/nll2vlypWqqamx5VavXq3BgwdrxowZ9dYRCoVO2/5ER0crNjZW0v8uw/Pz81VdXa0RI0bo3XffbTQfBEGTbj0cP3683njjDU2bNk3btm3T3XffrQkTJqhr165as2ZNveUvvfTSOq/kBw0apKSkJH300UeNbqtHjx6aMGFCo8v52rlzp1JTU5Wamqr+/fvrwQcf1OTJk+u9vZKVlaUBAwbYx0EQaPXq1Zo6daqCIFBeXp79mzBhgo4dO2bHfO3atercubNmzZpl+YSEBHtVfyqrV69WKBRq8PdUjZ07Z2ofx44dqyAImnRL82233aa//vWvGjp0qNavX69f/epXGj58uIYNG6YPP/yw3vLz5s2zc1iSvbpvyrkyc+ZMpaamNrocmo5ScFBTU6OVK1dq3Lhxys3N1Z49e7Rnzx6NGjVKhw8f1j/+8Q9bdu/evU16m+V0WL58uQYNGqS4uDh16NBBqamp+vvf/65jx46d1u2MHDlSOTk5Kigo0JYtW3TTTTepqKhIs2bN0gcffFBn2e7du9fLp6SkqKCgoNHt9OjR47Ttc0MyMzP18ssv65VXXtFrr72mzz77TC+++KI6dux4yv04evSoCgsL9eijj1qpnPg3b948Sf//i9R9+/apd+/e9X6I9+3bt9H927t3r7p06dKkt16+rLn2sTGzZ8/W5s2bVVBQoA0bNuiHP/yh/v3vf2vq1KkqLy+vs+yXz5WUlBRJahHnyrmI3yk42Lhxow4dOqSVK1c2eGtddna2vvvd756WbZ3sFWFNTU2dO3aeeuopzZ07V9OnT9fSpUuVlpam6Oho/fa3v9XevXtPy758WWxsrEaOHKmRI0eqT58+mjdvnp555pk6r2xPdldR0IT//fV03mnUkEgkoksvvdR5P07c2XLllVdqzpw5DWYGDRr01XfwK2hp+5iUlKTx48dr/PjxiomJ0fLly/XWW28pKyvLlmnJ58q5iFJwkJ2drbS0NP3xj3+s97WcnBw9++yzWrZsmeLj49WrVy/t2LHjlOs71VsBKSkpKiwsrPf5ffv2qWfPnvbxqlWr1LNnT+Xk5NRZX3PdIjtixAhJ0qFDh874tk7n224+UlNT1bZtW9XU1DRaKhkZGdqxY4eCIKiz3025175Xr15av3698vPzT3m10NDxaK599DFixAgtX778jJ8rX/d5crbj7aMmKisrU05OjqZMmaJZs2bV+7dkyRIVFRXZ++szZ87Utm3b9Oyzz9Zb14lXQJFIRJIa/OHfq1cvvfnmm6qsrLTPvfjii/rkk0/qLHfiVdYXX1W99dZbeuONN5r0uJp6S+qmTZsafOW2du1aSafnLYfGRCKRBo9Vc4mOjtbMmTO1evXqBgv/xO2skvS9731PBw8erDPCobS0VI8++mij25k5c6aCINBtt91W72tf/B40dDzO1D429ZbU0tLSk55769atk3Tmz5VTPa/QOK4UmmjNmjUqKirStGnTGvz66NGjlZqaquzsbF1++eVaunSpVq1apR/84AeaP3++hg8frvz8fK1Zs0bLli3T4MGD1atXLyUnJ2vZsmVq27atIpGIRo0apR49emjhwoVatWqVJk6cqMsuu0x79+7VU089Ve82zClTpignJ0czZszQ5MmTlZubq2XLlmnAgAEqLi5u9HE19ZbU6667TqWlpZoxY4b69eunyspKvf766/rb3/6mzMxMe7/6TBo+fLheeeUV/f73v1eXLl3Uo0cPGzMSCoWa9Di+qrvuukubNm3SqFGjtGjRIg0YMED5+fl699139corryg/P1+S7K++r776ar3zzjvq3LmzVqxYoYSEhEa3MW7cOF111VV64IEHtHv3bk2cOFG1tbXavHmzxo0bZ3OlTnY8zsQ+NvWW1NLSUl144YUaPXq0Jk6cqPPOO0+FhYV67rnntHnzZk2fPl1Dhw71OPJNd6rnFZrga7jj6aw0derUIC4uLigpKTnpMnPnzg1iYmLsVsDPP/88WLJkSdC1a9cgNjY26NatWzBnzpw6two+//zzwYABA4I2bdrUu43ud7/7XdC1a9cgHA4HF110UbB169Z6t6TW1tYGd955Z5CRkRGEw+Fg6NChwYsvvljvFr4g+Gq3pK5bty6YP39+0K9fvyAxMTGIjY0NevfuHVx33XXB4cOH663z2muvrbeOL99me7JbUidPntzgPuzcuTO4+OKLg/j4+ECSrauoqCiQFFxxxRWNPo6srKxg4MCBjS53sscQBEFw+PDh4Nprrw3OO++8ICYmJujUqVNwySWXBI8++mid5fbt2xdMmzYtSEhICDp27BjccMMNwUsvvdToLalB8L9bje+5556gX79+QWxsbJCamhpMmjSpzq2dJzseZ2Ifm3pLalVVVfDYY48F06dPt3MyISEhGDp0aHDPPffUufX0xDqfeeaZOus4cbvpF58LJ7sl9Z577mlwP071vMKphYKgCb/NAVqwtWvXasqUKdq2bZsuuOCCr3t3gLMav1PAWW/Tpk264oorKATgNOBKAQBguFIAABhKAQBgKAUAgKEUAACmyX+81tL/dDwqyr3ffP6XpuY6Dq3x9/+jR4/2yp34C1UXX5y62VSN/S9wp0s4HPbKffGvkZvqn//8p9e20Do15ecKVwoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDANPl/XmvpA/Fa8v4153C7tm3bOme+853vOGeGDRvmnJk0aZJzRpJ27drlnPE55omJic6ZDh06OGfy8vKcM5IUHx/vnPEZ8vfCCy84Z9asWeOc2b9/v3MGXw0D8QAATigFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAACYVjMQz4fPY2qu4XaLFy/2yvXp08c54zM0befOnc4Zn4FzkjRkyBDnTHl5uXMmEok4Z4qLi50zx48fd85IUmlpqXMmNTXVOePzmHr06OGc8Xk8kvSLX/zCOXPw4EGvbbU2DMQDADihFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIBpNVNSW/LE0x//+MfOmQ4dOnhtq7Cw0DlTVVXlnImKcn894TN9U5LC4bBzZsaMGc6Zzz77zDnjM+nTZ4KrJG3ZssU5M2nSJOfM9u3bnTM+E2YzMjKcM5LfVN/58+d7bau1YUoqAMAJpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAANPm696B06W5BuKdd955zpnu3bs7Zz766CPnjCQlJiZ65VyVlJQ4Z9LT0722tXfvXueMz/H7xje+4Zz5/PPPnTM+g+0k6eKLL3bOHDhwwDkTFxfnnImPj3fOlJWVOWckqVOnTs6Zq666yjmzYsUK54zv4NDmGs7ZFFwpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAANNqBuLV1tY2y3Z69+7tnKmurnbOtGnj960pLi52zoTDYedMdHS0c8Zn3yQpOTnZObN27VrnzJ133umc8Rnq5vu99ckdPnzYOROJRJwzSUlJzpnY2FjnjCRVVFQ4Z4YOHeqc8RmI15IG2/niSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAACYVjMQr7kMHDjQOVNeXu6c8RlS56ukpMQ54zMQr6amxjkj+Q1bO3TokHNmw4YNzhmfYYe+x2HPnj3OmVAo5Jzp1KmTc8ZnWF9cXJxzxtfIkSObbVtnO64UAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgGEgnqNu3bo5Z44dO+acac6BeEeOHHHOJCQkOGd8hqZJUmVlpXPGZ3Dh+++/75xp3769c+bgwYPOGUnq0qWLcyY5Odk5k56e7pzxGUDo8z2SpNzcXOdMfn6+cyY2NtY543OutjRcKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAAzDk9JdVnGqSPxMRE50xKSorXtnwmfVZVVTlnoqOjnTO+amtrnTMVFRXOGZ9j7jNJMxQKOWckvymznTt3ds74HDuf4+AzwdVXVJT7699BgwY5Z7Zu3eqcaWm4UgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAADmnB6I16NHD+dMcXGxcyYcDjtnIpGIc0aSgiBwzrRv3945ExMT45yJi4tzzvjyGYBWU1PjnPEZ1peamuqc8eVz7vkM3ktISHDOFBUVOWckv8dUXV3tnPH5+cBAPABAq0IpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAnNMD8bp37+6cKS8vd874DGfz5fOY9u3b55yprKx0zkRHRztnfHM+gwt9hqb5HG/f4+CzfxUVFc4Zn4F4nTt3ds6UlpY6ZySpqqqqWTJ9+vRxzrQGXCkAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAc04PxOvSpYtzpqamxjlz/Phx50w4HHbOSFJSUpJzpra21jnjMzTN59hJfgPkgiBwzvgcc599Kyoqcs5IUkpKinPGZ4BjfHy8c8bnHO/YsaNzRpIKCwudMz5DKYcMGeKcaQ24UgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAmHN6SmpiYqJzprKy0jlTUFDgnOnevbtzRpKef/5554zPcfCZQlpVVeWckfyml/pkYmJinDM+j8lnwqwkxcXFOWd8JuD6TFbduXOnc2batGnOGcnv3PN53voc79aAKwUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgzumBeD5D08rKypwz1dXVzplQKOSckaQPPvjAOTNmzBjnTHFxsXPGV01NjXMmOTnZOeMzuNBnOJvP+SD5Dd/zPY9c/fe//3XOJCQkeG3L5zFVVFQ4Z3zOodaAKwUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgWs1AvDZt3B9KbGyscyY6Oto548Nn+JkkHTx40DnTXEPT4uPjvXI+A/EikYhz5vPPP3fO+AzE88lIzTcQz+cc3717t3PGdyBeVJT7a1mfnw8+51BiYqJzRmreAZON4UoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAmFYzEK9jx47OGZ9hYT7DzHyGcVVWVjpnfLflk6murnbOhMNh54wk5efnO2dKS0udMzExMc4ZnyF/R44ccc5IfoMBfc5xn+0cOnSoWbbjq6yszDnj81zv1KmTc0aS9uzZ45U7E7hSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAKbVDMRLTk52zvgMgisvL3fO+OzbJ5984pyRpKKiIudMJBJxznz22WfOGZ/jLUlRUe6vXXyGrcXFxTlnfAbi+Q6C8xlC6HPMExMTmyXjOxiwtrbWOeNzHHzOu7S0NOeMxEA8AEALRSkAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAA02qmpAZB4JwpLi52zlRUVDhn+vTp45zZuXOnc0bye0w+0zd9REdHe+ViYmKcMz7ng88E3LKyMueMzzRWyW9qp4/27ds7Z0pKSpwz27dvd85IUtu2bZ0zBQUFzhmfaaw+02JbGq4UAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgGk1A/E6dOjgnPEZBBcfH++cSU5Ods68//77zhlJSk1Ndc74DBjz0aaN3+kWDoedMz4D2mpqapwzPkPTfM4hyW+gYFVVlXPG5zF1797dObN3717njCRdeOGFzhmfY+4zlDIpKck509JwpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAABMqxmIN2zYMOeMz5Asn0x6erpzpqCgwDkjSSNGjHDOlJaWOmd8hqb5ZCS/QXCVlZXNsh2fTFSU32uxioqKZsn4DIocPHiwc+bYsWPOGUkqKytzzsTFxTlnIpGIc8bn+SdJq1at8sqdCVwpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAANNqBuKVlJQ4Z3yGZHXt2tU507ZtW+fMe++955yRpCFDhjhnCgsLnTMJCQnOGV+hUMg5Ew6HnTM+w+1qamqcMz7nquQ35M9nuJ3P4MLMzEznzJo1a5wzkvT44487Z55++mnnjM/36dChQ86ZloYrBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGBCQRAETVrQYyhZa5SYmOic6dmzp3Nmx44dzhlJWrp0qXOmoKDAOeMzcC4pKck5I0kHDhxwznTs2NE5ExMT45zxOR8+/fRT54yvzp07O2fS0tKcMz169HDOzJs3zzkj+Q0GLC4uds6Ul5c7Z1q6pvy450oBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGDafN07cLbxmbb4/vvvO2fatm3rnJGkDh06OGfy8/OdM23auJ86hw8fds5IUnx8vHPG5zj4TAL2mdjZxMHE9fhMpq2oqPDalquEhATnzODBg722tW7dOq8cmoYrBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGDO6YF4PgPQoqLce7SmpsY58+1vf9s5I0lVVVVeOVdlZWXOGZ9jJ0m9e/d2zuTm5npty1V6erpzxue8k6S4uDjnTGlpqXPG53t74MAB50xWVpZzRvIbiOdzzH0HF57tuFIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAA5pweiOcz8MpnuJ2Pvn37euWOHTvmnImNjXXO+ByHPn36OGck6eOPP3bOlJSUOGe6dOninPEZUuc7GDA+Pt454zMIrrKyslkynTp1cs748nmun6tD9LhSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAOacHojnIzo62jnjMzwuIyPDOSP5DbfbvXu3c6a2ttY5s2vXLueMJOXn5ztnBgwY4JzxeUwxMTHOGd+hikVFRc6Z5hqQGA6HnTMJCQnOGd9tVVRUOGcYiAcAOOdRCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMCEgiaO9fOZGNgaRUW596jP9M34+HjnjCQtXbrUOXPRRRc5Z5KTk50zubm5zhlJqqqqcs74HL+jR486Z1JSUpwzJSUlzhlJat++vXMmPT3dOeMzWTUvL88588gjjzhnJOm1117zyqFpU1y5UgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGgXjw0r17d+fMgAEDvLblM9QtKSnJOeMz7NBHZWWlV666uto5s3//fufMv/71L+dMcXGxcwbNj4F4AAAnlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAEybpi7YxLl5AICzGFcKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAA839RQWPyqla+4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_test_image(i):\n",
    "    plt.imshow(test_images[i], cmap='gray')\n",
    "    plt.title(f\"Actual: {class_names[test_labels[i]]}, Predicted: {class_names[predicted_classes[i]]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a few test images with predictions\n",
    "for i in range(5):  # Display first 5 images and predictions\n",
    "    display_test_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0eae0a4-195c-4610-a6b5-c28fbdc67a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 233248 bytes, or 0.22 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(model_file_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4403d7-2b95-439b-be26-77279b209c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a1390>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a1390>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3640>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3640>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3f70>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3f70>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e8a30>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e8a30>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1ea320>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1ea320>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e93c0>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e93c0>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5d000>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5d000>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5e200>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5e200>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5f1c0>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5f1c0>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b90220>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b90220>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b91210>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b91210>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b92230>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b92230>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a1390>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a1390>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3640>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3640>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3f70>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1a3f70>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e8a30>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e8a30>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1ea320>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1ea320>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e93c0>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f514f1e93c0>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5d000>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 1, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5d000>, 139988210926896), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5e200>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5e200>, 139988210907504), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5f1c0>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b5f1c0>, 139988211106320), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b90220>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(32,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b90220>, 139988211106160), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b91210>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(3, 3, 32, 16), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b91210>, 139988211088832), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b92230>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(16,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f5135b92230>, 139988211088912), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpun89bxse/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpun89bxse/assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [lite.Optimize.DEFAULT]\n",
    "\n",
    "# Generate a quantized model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open(quantized_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Get the size of the model\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Quantized Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a4022-3231-4d8a-902b-f3fd782568c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tflite_model.predict(test_images.reshape(-1, 28, 28, 1))  # Reshape for the CNN input\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8b114-dbcf-49fe-becd-b63c2deb1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_image(i):\n",
    "    plt.imshow(test_images[i], cmap='gray')\n",
    "    plt.title(f\"Actual: {class_names[test_labels[i]]}, Predicted: {class_names[predicted_classes[i]]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a few test images with predictions\n",
    "for i in range(5):  # Display first 5 images and predictions\n",
    "    display_test_image(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
